"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"why-sql-for-infrastructure","metadata":{"permalink":"/blog/why-sql-for-infrastructure","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/deep-dives/why-sql-for-infrastructure.mdx","source":"@site/blog/deep-dives/why-sql-for-infrastructure.mdx","title":"Why SQL is right for Infrastructure Management","description":"Infrastructure is the heart of your company. Without it, nothing can actually be done and there\'d be no reason for customers to pay you. For software companies that infrastructure is the software that its engineers directly write, and usually the cloud infrastructure and services it runs on top of and integrates with.","date":"2023-04-06T00:00:00.000Z","formattedDate":"April 6, 2023","tags":[{"label":"deep-dives","permalink":"/blog/tags/deep-dives"}],"readingTime":29.51,"hasTruncateMarker":true,"authors":[{"name":"David Ellis","imageURL":"https://github.com/dfellis.png","key":"dfellis"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"why-sql-for-infrastructure","title":"Why SQL is right for Infrastructure Management","authors":["dfellis","depombo"],"date":"2023-04-06T00:00:00.000Z","image":"/home/iasql-connector.gif","tags":["deep-dives"]},"nextItem":{"title":"Deploy Stable Diffusion in EC2 using a SQL query","permalink":"/blog/deploy-stable-diffusion"}},"content":"Infrastructure is the heart of your company. Without it, nothing can actually be done and there\'d be no reason for customers to pay you. For software companies that infrastructure is the software that its engineers directly write, and usually the cloud infrastructure and services it runs on top of and integrates with.\\n\\nIn this post, we will geek out on various software abstractions and data structures, tools used by professionals of various sorts, and dig into the pros and cons of these with respect to cloud infrastructure management in particular. We\'ll see that while SQL has its own warts, it is the \\"least worst\\" of all of the options out there.\\n\\n\x3c!--truncate--\x3e\\n\\n## Following these instructions is imperative\\n\\nThe simplest way to define how to set something up is to write up a list of instructions to follow, in order, to build whatever infrastructure you\'re dealing with, whether its the instructions on how to build a restaurant or an AWS Fargate cluster. This list of steps to process (with a LISt Processor, or [LISP](https://en.wikipedia.org/wiki/Lisp_%28programming_language%29), if you will \ud83d\ude09), which can include instructions to repeat or skip over steps based on conditions you have run into, is called [imperative programming](https://en.wikipedia.org/wiki/Imperative_programming) in the software world.\\n\\n:::note Imperative Restaurant Instructions\\n\\n![Man in hard-hat builds your restaurant](/img/why-sql-for-infrastructure/man-in-hard-hat-builds-your-restaurant.png)\\n\\n1. Design restaurant.\\n2. Flatten the ground.\\n3. Prop up pieces of wood next to each other.\\n4. Bang nails into wood.\\n5. Is restaurant finished? No, go to 3.\\n\\n:::\\n\\nFor your cloud infrastructure, this is similar to using the [AWS SDK](https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/) and directly calling the various methods, checking their results and eventually arriving at the desired infrastructure. It is *also* like following a step-by-step guide clicking through the various AWS console UI elements to set up your infrastructure, like in [this guide](https://towardsdatascience.com/deploy-your-python-app-with-aws-fargate-tutorial-7a48535da586). The latter may not be automated, to you, but to your company executives it is as they don\'t care how the infrastructure building was accomplished as long as it was done quickly, cheaply, and reliably, and you know they always want all three. \ud83d\ude09\\n\\nImperative infrastructure management works *well* for initial setup of new infrastructure. There\'s nothing there to interfere with it, so you can just write the \\"happy path\\" and get it working. And if that fails, you can just tear it all down and start all over again if that\'s cheap, which it is for cloud infrastructure (though not for building a restaurant), so the [cyclomatic complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity) of the code you write is low and everyone can follow along with it. For example, one can use the [`aws` cli application](https://aws.amazon.com/cli/) inside of a [`bash` script](https://www.gnu.org/software/bash/) to create a new [ssh keypair](https://en.wikipedia.org/wiki/Ssh-keygen) in AWS, a [security group](https://docs.aws.amazon.com/vpc/latest/userguide/security-groups.html) enabling ssh access, a new EC2 instance associated with both, and then access that instance (to demonstrate that it works).\\n\\n```bash\\n#!/bin/bash\\n\\naws ec2 create-key-pair \\\\\\n  --region us-west-2 \\\\\\n  --key-name login \\\\\\n  --key-type ed25519 | \\\\\\n  jq -r .KeyMaterial > login.pem\\nchmod 600 login.pem\\nSG_ID=$(aws ec2 create-security-group \\\\\\n  --group-name login-sg \\\\\\n  --description \\"Login security group\\" \\\\\\n  --region us-west-2 \\\\\\n  --vpc-id $(aws ec2 describe-vpcs \\\\\\n    --region us-west-2 | \\\\\\n    jq -r \'.Vpcs[] | select(.IsDefault = true) | .VpcId\') | \\\\\\n  jq -r .GroupId)\\naws ec2 authorize-security-group-ingress \\\\\\n  --group-name login-sg \\\\\\n  --region us-west-2 \\\\\\n  --ip-permissions \'IpProtocol=tcp,FromPort=22,ToPort=22,IpRanges=[{CidrIp=0.0.0.0/0}]\' > /dev/null\\nINSTANCE_ID=$(aws ec2 run-instances \\\\\\n  --region us-west-2 \\\\\\n  --subnet-id $(aws ec2 describe-subnets \\\\\\n    --region us-west-2 | \\\\\\n    jq -r .Subnets[0].SubnetId) \\\\\\n  --instance-type t3.small \\\\\\n  --image-id resolve:ssm:/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id \\\\\\n  --security-group-ids ${SG_ID} \\\\\\n  --key-name login \\\\\\n  --associate-public-ip-address \\\\\\n  --tag-specifications \'ResourceType=instance,Tags=[{Key=name,Value=login-inst}]\' \\\\\\n  --output json | \\\\\\n  jq -r .Instances[0].InstanceId)\\necho EC2 instance ${INSTANCE_ID} starting \\naws ec2 wait instance-status-ok \\\\\\n  --region us-west-2 \\\\\\n  --instance-ids ${INSTANCE_ID}\\nPUBLIC_IP=$(aws ec2 describe-instances \\\\\\n  --region us-west-2 \\\\\\n  --filters Name=tag:name,Values=[login-inst] \\\\\\n  --filters Name=instance-state-name,Values=[running] | \\\\\\n  jq -r .Reservations[0].Instances[0].PublicIpAddress)\\necho Server accessible at ${PUBLIC_IP}\\nssh -i login.pem -o \\"StrictHostKeyChecking no\\" ubuntu@${PUBLIC_IP} uname -a\\n```\\n\\nKeep in mind that this is a *simple* script that does not handle failure of any of the commands called, creating one EC2 instance with two supporting elements (a security group to allow access to the SSH port and an SSH keypair to log into the instance). If you intend to actually use it multiple times, several of the arguments currently hardwired should be turned into shell arguments themselves, and all of the error paths should be tackled.\\n\\nHowever, the vast majority of the time you are not creating clones of the same infrastructure over and over again, instead you need to make changes to your existing infrastructure, and that original code is more than likely useless to you. To get from the *current* state of your infrastructure to your *desired* state, you need to call different APIs than you did before, and its much riskier to make a mistake because this infrastructure is already in use.\\n\\n## Declare your intentions at once\\n\\nWhen the desired state is relatively simple to define and the mechanism to reach that state is not that important, writing up a declaration of what is needed and letting something/someone else deal with it is the most logical abstraction. This would be like drafting up the architectural draft for your new restaurant and paying a contracting company to actually build it, or [writing HTML and letting a web browser render it](https://en.wikipedia.org/wiki/Declarative_programming#Domain-specific_languages), or writing a [Terraform HCL](https://github.com/hashicorp/hcl) file and letting the Terraform CLI tool `apply` it. This is called [declarative programming](https://en.wikipedia.org/wiki/Declarative_programming) in the software world, and has many advantages (and a few disadvantages!) for cloud infrastructure management.\\n\\n:::note Declarative Restaurant Instructions\\n\\n![King in robes on throne](/img/why-sql-for-infrastructure/king-in-robes-on-throne.png)\\n\\n*By decree of the king, a restaurant shall be built!*\\n\\n:::\\n\\nIn declarative programming you have some initial state and in comparatively dense and high-level declarative code define the desired state. In many use-cases (like web browsers and sometimes for restaurant-building contractors) the initial state is \\"blank\\" and the engine that transforms the declarative code into imperative operations can often be a relatively straightforward [interpreter](https://en.wikipedia.org/wiki/Interpreter_%28computing%29) walking the [AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree) of the declarative code.\\n\\nInfrastructure management *generally* is not that straightforward. Changes to infrastructure require in-place mutations of the current state or a full [blue/green deployment](https://en.wikipedia.org/wiki/Blue-green_deployment) of the entirety of your production infrastructure is very expensive, requiring all resource costs to be doubled, and deployments to require some amount of downtime for users to swap over and state to be resynchronized. Particularly for databases this approach is essentially impossible as there is too much state to swap, which is what makes database migrations tricky when dropping data.\\n\\nSo declarative programming for infrastructure, also known as [Infrastructure as Code](https://en.wikipedia.org/wiki/Infrastructure_as_code), must take the existing state and generate the imperative operations to perform based on the differences between the existing state and the declared desired state. This is like a contracting company being given an architecture draft for a new restaurant and being told to remodel an existing commercial space (maybe it was also a restaurant, maybe it was a clothing store, etc) and the contracting company figuring out what needs to be torn down first, what can be re-used, and what needs to be built new.\\n\\n```hcl\\nterraform {\\n  required_providers {\\n    aws = {\\n      source = \\"hashicorp/aws\\"\\n      version = \\"4.60.0\\"\\n    }\\n  }\\n}\\n\\ndata \\"external\\" \\"example\\" {\\n  program = [\\n    \\"bash\\",\\n    \\"-c\\",\\n    <<-EOT\\n      if [ ! -f ./login.pem ]; then\\n        # Terraform doesn\'t support creating the PEM file via AWS. It has to be\\n        # created locally and then imported into AWS. Inside of a conditional so\\n        # we don\'t accidentally re-create the file between \'plan\' and \'apply\'\\n        yes \'\' | ssh-keygen -t ed25519 -m PEM -f login.pem\\n      fi\\n      chmod 600 login.pem\\n      # The output of this script needs to be JSON formatted. This is a bit wonky\\n      # but building it over time via \'jq\' operators is actually harder to follow\\n      echo {\\n      echo   \'\\"access_key_id\\": \\"\'$(aws configure get aws_access_key_id)\'\\",\'\\n      echo   \'\\"secret_access_key\\": \\"\'$(aws configure get aws_secret_access_key)\'\\",\'\\n      echo   \'\\"public_key\\": \\"\'$(cat login.pem.pub)\'\\"\'\\n      echo }\\n    EOT\\n  ]\\n}\\n\\nprovider \\"aws\\" {\\n  region = \\"us-west-2\\"\\n  access_key = data.external.example.result[\\"access_key_id\\"]\\n  secret_key = data.external.example.result[\\"secret_access_key\\"]\\n}\\n\\nresource \\"aws_key_pair\\" \\"login\\" {\\n  key_name = \\"login\\"\\n  public_key = data.external.example.result[\\"public_key\\"]\\n}\\n\\nresource \\"aws_security_group\\" \\"login_sg\\" {\\n  name = \\"login-sg\\"\\n  description = \\"Login security group\\"\\n  ingress {\\n    from_port = 22\\n    to_port = 22\\n    protocol = \\"tcp\\"\\n    cidr_blocks = [\\"0.0.0.0/0\\"]\\n  }\\n}\\n\\nresource \\"aws_instance\\" \\"login\\" {\\n  ami = \\"resolve:ssm:/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id\\"\\n  instance_type = \\"t3.small\\"\\n  associate_public_ip_address = true\\n  key_name = aws_key_pair.login.key_name\\n  vpc_security_group_ids = [aws_security_group.login_sg.id]\\n  tags = {\\n    name = \\"login-inst\\"\\n  }\\n}\\n```\\n\\nThis example still requires the following lines from the imperative example to get the public IP address of the new EC2 instance and log into it:\\n\\n```bash\\nPUBLIC_IP=$(aws ec2 describe-instances \\\\\\n  --region us-west-2 \\\\\\n  --filters Name=tag:name,Values=[login-inst] \\\\\\n  --filters Name=instance-state-name,Values=[running] | \\\\\\n  jq -r .Reservations[0].Instances[0].PublicIpAddress)\\necho Server accessible at ${PUBLIC_IP}\\nssh -i login.pem -o \\"StrictHostKeyChecking no\\" ubuntu@${PUBLIC_IP} uname -a\\n```\\n\\nThis diffing of original and desired state can be resolved several ways. The contracting company could spend a lot of time inspecting the existing commercial space to find absolutely every piece that can be re-used, removing them and setting them aside with the materials that need to be purchased, then rework the interior walls, and build everything from scratch, or it could decide to tear everything out back into an empty space and rebuild from scratch (like what your web browser does between web pages), or perhaps you need to keep the space mostly usable for customers, minimizing disruption to the current operations while things are changed. This last case is what infrastructure management tools need to tackle your infrastructure with the least amount of downtime or no downtime at all, if you\'re *careful* with it.\\n\\nHow can you \\"careful\\" with declarative programming tools, if you don\'t have direct control over what they do? Terraform does this with a [plan](https://developer.hashicorp.com/terraform/cli/commands/plan) command, which performs the diffing of current and desired state and reports back to you the operations it expects to execute to do so. Terraform will tell you when a change it intends to make is going to provision new resources, alter existing resources, drop resources, and *replace* resources. That last one is particularly annoying when it shows up because it means the change you want to perform can only be done by dropping it and recreating it with the new configuration.\\n\\n```bash\\n$ terraform plan\\n\\ndata.external.example: Reading...\\ndata.external.example: Read complete after 1s [id=-]\\n\\nTerraform used the selected providers to generate the following execution\\nplan. Resource actions are indicated with the following symbols:\\n  + create\\n\\nTerraform will perform the following actions:\\n\\n  # aws_instance.login will be created\\n  + resource \\"aws_instance\\" \\"login\\" {\\n      + ami                                  = \\"resolve:ssm:/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id\\"\\n      + arn                                  = (known after apply)\\n      + associate_public_ip_address          = true\\n      + availability_zone                    = (known after apply)\\n      + cpu_core_count                       = (known after apply)\\n      + cpu_threads_per_core                 = (known after apply)\\n      + disable_api_stop                     = (known after apply)\\n      + disable_api_termination              = (known after apply)\\n      + ebs_optimized                        = (known after apply)\\n      + get_password_data                    = false\\n      + host_id                              = (known after apply)\\n      + host_resource_group_arn              = (known after apply)\\n      + iam_instance_profile                 = (known after apply)\\n      + id                                   = (known after apply)\\n      + instance_initiated_shutdown_behavior = (known after apply)\\n      + instance_state                       = (known after apply)\\n      + instance_type                        = \\"t3.small\\"\\n      + ipv6_address_count                   = (known after apply)\\n      + ipv6_addresses                       = (known after apply)\\n      + key_name                             = \\"login\\"\\n      + monitoring                           = (known after apply)\\n      + outpost_arn                          = (known after apply)\\n      + password_data                        = (known after apply)\\n      + placement_group                      = (known after apply)\\n      + placement_partition_number           = (known after apply)\\n      + primary_network_interface_id         = (known after apply)\\n      + private_dns                          = (known after apply)\\n      + private_ip                           = (known after apply)\\n      + public_dns                           = (known after apply)\\n      + public_ip                            = (known after apply)\\n      + secondary_private_ips                = (known after apply)\\n      + security_groups                      = (known after apply)\\n      + source_dest_check                    = true\\n      + subnet_id                            = (known after apply)\\n      + tags                                 = {\\n          + \\"name\\" = \\"login-inst\\"\\n        }\\n      + tags_all                             = {\\n          + \\"name\\" = \\"login-inst\\"\\n        }\\n      + tenancy                              = (known after apply)\\n      + user_data                            = (known after apply)\\n      + user_data_base64                     = (known after apply)\\n      + user_data_replace_on_change          = false\\n      + vpc_security_group_ids               = (known after apply)\\n    }\\n\\n  # aws_key_pair.login will be created\\n  + resource \\"aws_key_pair\\" \\"login\\" {\\n      + arn             = (known after apply)\\n      + fingerprint     = (known after apply)\\n      + id              = (known after apply)\\n      + key_name        = \\"login\\"\\n      + key_name_prefix = (known after apply)\\n      + key_pair_id     = (known after apply)\\n      + key_type        = (known after apply)\\n      + public_key      = \\"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIF4yiy4lef2WFQ7zdtuOZUHz9onyADTJ16l0OX8a211y damocles@zelbinion\\"\\n      + tags_all        = (known after apply)\\n    }\\n\\n  # aws_security_group.login_sg will be created\\n  + resource \\"aws_security_group\\" \\"login_sg\\" {\\n      + arn                    = (known after apply)\\n      + description            = \\"Login security group\\"\\n      + egress                 = (known after apply)\\n      + id                     = (known after apply)\\n      + ingress                = [\\n          + {\\n              + cidr_blocks      = [\\n                  + \\"0.0.0.0/0\\",\\n                ]\\n              + description      = \\"\\"\\n              + from_port        = 22\\n              + ipv6_cidr_blocks = []\\n              + prefix_list_ids  = []\\n              + protocol         = \\"tcp\\"\\n              + security_groups  = []\\n              + self             = false\\n              + to_port          = 22\\n            },\\n        ]\\n      + name                   = \\"login-sg\\"\\n      + name_prefix            = (known after apply)\\n      + owner_id               = (known after apply)\\n      + revoke_rules_on_delete = false\\n      + tags_all               = (known after apply)\\n      + vpc_id                 = (known after apply)\\n    }\\n\\nPlan: 3 to add, 0 to change, 0 to destroy.\\n\\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\nNote: You didn\'t use the -out option to save this plan, so Terraform can\'t\\nguarantee to take exactly these actions if you run \\"terraform apply\\" now.\\n```\\n\\nWhen you see steps where Terraform plans to drop or replace resources, you can decide if that\'s actually alright, or if it will negatively impact your business during the deployment, and then you can abort and define an intermediate state to first transition to, usually creating a new resource without dropping the old one, then do whatever operation is necessary to move internal state to it, then continue by deleting the old resource. This is like checking up on your contracting company before they start doing their work to make sure they don\'t do anything strange you don\'t want, and sometimes needing to handhold them through details of your business to come up with the new plan of action.\\n\\nUntil cloud resource management is as quick to execute as a webpage render where state changes can be done faster than a human can respond, this sort of hand-holding is necessary for live infrastructure with uptime guarantees. Declarative programming, at least for cloud infrastructure management, is a [leaky abstraction](https://en.wikipedia.org/wiki/Leaky_abstraction) over the imperative operations that need to be executed.\\n\\nThe Terraform HCL files don\'t have the prior state encoded into them, nor does it depend on being stored within a git repository to provide that prior state, so how does Terraform get that initial state to operate on? Terraform maintains an [internal statefile](https://developer.hashicorp.com/terraform/language/state) to perform the diffing against. It has a mechanism to [refresh that state](https://developer.hashicorp.com/terraform/cli/commands/refresh), but being a JSON blob with poor typing, can [become corrupted, requiring manual intervention](https://faun.pub/cleaning-up-a-terraform-state-file-the-right-way-ab509f6e47f3).\\n\\nWhy does Terraform need this JSON-based statefile when it has the much cleaner HCL format that you use as a developer? This is because HCL, as a language that provides affordances to the developer\'s own intuition on how to structure their representation of their infrastructure, does not have a [canonical form](https://en.wikipedia.org/wiki/Canonical_form) to make these comparisons with. Terraform must first pre-process the HCL into an internal data structure which it can then generate a [diff](https://en.wikipedia.org/wiki/Diff) that can then be used to determine the operations to perform. The JSON-based statefile is likely a very close representation of this internal data structure, and manual editing of it is highly discouraged because the details of it likely vary from release to release with an internal migration mechanism performed on upgrade of the Terraform CLI.\\n\\nNeeding to learn Terraform\'s HCL language to make infrastructure changes has been cited as a [weakness](https://www.pulumi.com/why-pulumi/) that can be mitigated by wrapping it in the syntaxes more familiar to the user like [Terraform CDK](https://developer.hashicorp.com/terraform/cdktf), [Pulumi](https://www.pulumi.com), or [AWS CDK](https://docs.aws.amazon.com/cdk/v2/guide/home.html). This *is* a weakness, but we believe that the HCL-to-Statefile transformation being [surjective](https://en.wikipedia.org/wiki/Surjective_function) instead of [bijective](https://en.wikipedia.org/wiki/Bijection) is actually the bigger issue.\\n\\n:::note Surjective Function *f* Diagram\\n\\n![Surjective Diagram](https://upload.wikimedia.org/wikipedia/commons/6/6c/Surjection.svg)\\n\\n[*From Wikipedia*](https://en.wikipedia.org/wiki/Surjective_function#/media/File:Surjection.svg) A function *f* is surjective if all of its inputs *X* map to all outputs *Y*, but overlaps can occur.\\n\\nThis is like multiple Terraform HCL representations all mapping to the exact same cloud infrastructure.\\n\\n:::\\n\\n\x3c!-- TODO: This section could ideally show some way you can trigger a corruption of the statefile vs AWS\'s actual state --\x3e\\n\\n## The Objective is Bi- hmmmm....\\n\\n:::note Bijective Function *f* Diagram\\n\\n![Bijective Diagram](https://upload.wikimedia.org/wikipedia/commons/a/a5/Bijection.svg)\\n\\n[*From Wikipedia*](https://en.wikipedia.org/wiki/Bijection#/media/File:Bijection.svg) A function *f* is bijective if all of its inputs *X* map to all outputs *Y* exactly once.\\n\\n:::\\n\\nA bijective function has a one-to-one mapping of all state in one set with all state in another set. Both representations are equivalent (given the appropriate transform functions), both sets (of possible cloud states) are the same size (conceptually), and for any one particular state in one set there is exactly one state in the other set. Programming Languages don\'t fit the bill here, as was demonstrated earlier, but what does? What, exactly, defines your cloud resources in the most minimal way?\\n\\nThe best answer in most cases is ~~[water vapor](https://en.wikipedia.org/wiki/Cloud)~~ [Entities](https://en.wikipedia.org/wiki/Entity#In_computer_science). Each cloud resource has some unique identifier and a set of properties that configure it. Some of those properties can be relations or dependencies on other cloud resources. The ordering of the entities does not matter, so long as the dependencies are explicitly defined to make determination of the order of operations possible. You may have noticed that this sounds a lot like [Objects](https://en.wikipedia.org/wiki/Object_%28computer_science%29) in [Object-Oriented Programming](https://en.wikipedia.org/wiki/Object-oriented_programming). It is very much the same, with the only difference being that the unique identifier *must* be unique, so if we considered an array of pointers to these objects as the entity IDs, multiple pointers to the same object wouldn\'t be allowed.\\n\\nThis means the representation of your cloud resources should be *data*, not code. Particularly, it should be [relational data](https://en.wikipedia.org/wiki/Relational_model) to allow the references/relations with other entities that actually exist within your cloud. Modeling each entity as a wholly-siloed object without those relations explicitly tracked *might* work for read-only cases, but puts the burden of properly joining the entities on the user and could lead to unexpected errors during mutation if the disjointed state is not carefully kept in sync. This mutation of that data would be the code, and fits very well into a classic [REST architecture](https://www.redhat.com/en/blog/rest-architecture) (in the original sense, though it could also work in the [Web-oriented RESTful meaning](https://en.wikipedia.org/wiki/Overview_of_RESTful_API_Description_Languages#Hypertext-driven_API).\\n\\nSince your infrastructure is actually data, just about any data format *could* be used to store it, as demonstrated by Terraform\'s usage of JSON under-the-hood. Some will require more application-level guarantees on top of the data format to achieve all of the necessary components, however. For instance JSON does not have any native way to represent *references* to other JSON objects that are not wholly contained within the object in question, so you would need to define a way to represent them and enforce it within your application. [YAML does support them via tags](https://yaml.org/spec/1.2.2/#24-tags) but being a markup language has multiple equivalent representations and is therefore not suitable for this purpose (without a strict linter to reduce it back down to a canonical form). There are also [binary formats that can encode references](https://capnproto.org/language.html#dynamically-typed-fields) with [SQLite\'s file format](https://www.sqlite.org/cli.html) arguably being the most prolific.\\n\\nSQL immediately stands out here because it was designed for making [relational algebra](https://en.wikipedia.org/wiki/Relational_algebra), the other side of the [Entity-Relationship model](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model), accessible. There are likely more people who know SQL than any programming language (for IaC) or data format you could choose to represent your cloud infrastructure. Many non-programmers know it, as well, such as data scientists, business analysts, accountants, etc, and there is an entire ecosystem of useful functionality you gain \\"for free\\" by using a SQL representation than just about any format out there. Furthermore, a powerful SQL engine like the open source [PostgreSQL](https://postgresql.org) provides tools capable of elevating the infrastructure management *experience* beyond what IaC can do today.\\n\\n### The Advantages of SQL Databases for Infrastructure Management\\n\\nFirst, by defining [foreign key relations](https://www.postgresql.org/docs/current/tutorial-fk.html) between tables, the database can immediately inform you if you have dependencies that must be created before you can create the resource you desire. It will simply return an error message to you if you have inserted an incomplete record missing any required fields.\\n\\n\\"But,\\" you say, \\"programming languages with solid type systems, like [Rust](https://www.rust-lang.org), can also do that for you.\\" That is true, and this was not an attempt to disparage them, merely to show that you aren\'t losing anything by using SQL as the representation. There are, however, ways SQL goes above and beyond. By defining [unique, not-null, and check constraints](https://www.postgresql.org/docs/current/ddl-constraints.html) on top of a [rich type system](https://www.postgresql.org/docs/current/datatype.html) the database can prevent you from inserting faulty data to a greater degree than even Rust, as the constraints can be against the live state of your infrastructure itself. Instead of code needing to deal with unexpected values further down the line, the faulty data can be prevented from ever being inserted. Rust\'s compiler making sure that you handle every possible branch is an awesome thing, but SQL has the power to make more of these branches impossible in the first place.\\n\\nBy being in a database, you can answer questions about your infrastructure via queries, and in the case of security vulnerabilities or expensive misconfigurations you can update that state across the board at the same time. The shared, live nature of the SQL database more closely matches the reality of the cloud infrastructure, while the more malleable nature of the querying in the Structured Query Language lets you cross-check in many dimensions the AWS console does not.\\n\\nThere\'s a further advantage by having a bijective relationship with your cloud: you can re-synchronize the database with your cloud at any time, avoiding statefile issues by never getting too out-of-date, and being almost impossible to corrupt. Software bugs are unavoidable, but since populating the database is trivial (compared to the cloud account itself), it could simply be dropped and recreated at any time without an impact on your own usage.\\n\\nMost SQL databases have the concept of [trigger functions](https://www.postgresql.org/docs/current/sql-createtrigger.html#SQL-CREATETRIGGER-EXAMPLES) that execute based on changes to the database itself. With a collection of automatically-created trigger functions, an [audit log](https://en.wikipedia.org/wiki/Audit_trail) can be maintained, which can keep track of who made what cloud changes and when they did, going a step further than even a git repository of IaC changes, as changes done manually through the AWS console will also show up (though these would not have a database user to tie them to).\\n\\nTrigger functions have other uses that don\'t even have an analogous concept in IaC tooling. For instance, trigger functions can be added to automatically re-insert deleted records of sensitive cloud resources for automated [Chaos Engineering](https://en.wikipedia.org/wiki/Chaos_engineering) recovery. Either a mutation that was attempted by a developer that shouldn\'t be allowed (without also going through the much larger change of removing said trigger function first) so the resource is never deleted in the first place, as well as automatic recreation of the desired resource in the cloud if deleted via the AWS console or dropped during an outage.\\n\\nSQL Database [Snapshotting](https://en.wikipedia.org/wiki/Snapshot_%28computer_storage%29) could be used to quickly revert all infrastructure changes for infrastructure without hidden state (eg, the snapshot of your load balancers\' prior state would successfully restore the load balancer configuration, but a dropped database restored via a snapshot would not restore the data within, only that you had such a database in the first place and you would still need to manually re-load a backup snapshot of those, as well).\\n\\nDatabase engines also represent a standardized protocol to interface with your data. This standard opens up a world of possibilities that can encompass the imperative and declarative infrastructure management styles and go beyond them. You can:\\n  - Perform operations on your database via scripts and the [`psql` CLI](https://www.postgresql.org/docs/current/app-psql.html) tool, using whatever mix of declarative and imperative styles you prefer, mutating various tables declaratively and calling [`AWS SDK functions directly`](/docs/modules/aws/aws_sdk/?view=code-examples) where and when you see fit.\\n  - Integrate the database into your application itself with a [postgres client library](https://node-postgres.com/) allowing your applications to make infrastructure changes (like provisioning sharded resources for a client that wants isolation, or using a more accurate forecasting model to pre-allocate more resources before the storm hits).\\n  - Connect with a [SQL IDE](https://www.jetbrains.com/datagrip/features/postgresql/) for a variety of reasons, such as outage mitigation or inspection of your infrastructure in a tabular (Excel-like) view.\\n  - Connect the database to a [graphical dashboard](https://grafana.com/docs/grafana/latest/datasources/postgres/) for live visualization of your infrastructure\'s state and any other metadata transferred through.\\n  - Connect it to a [low code tool](https://docs.retool.com/docs/postgresql-integration) for rapid response dashboards for your infrastructure with buttons you can tackle known outage vectors near-instantly, or just to make self-service provisioning of new microservices in the infrastructure team\'s preferred way.\\n\\nThere are negatives to representing your infrastructure as SQL, of course, as there is no perfect solution for all situations, but we believe [IaSQL](https://iasql.com) is a superset versus IaC in all but one category.\\n\\nSQL is an old, irregular language to work with, but it is better known than HCL and SQL already has it\'s own Pulumi/CDK in the form of every ORM with introspection (like [Javascript\'s Prisma](https://www.prisma.io/docs/concepts/components/introspection), [Python\'s Django](https://docs.djangoproject.com/en/4.1/howto/legacy-databases/), [Go\'s XO](https://github.com/xo/xo) etc) and QueryBuilder ([LINQ](https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/), [Knex](https://knexjs.org/), etc) in whatever programming language you prefer. You probably already know it.\\n\\nPeer review of changes is *different* versus IaC. Instead of writing HCL or other IaC code, having it reviewed, and then `apply`ing it, you can enter an [IaSQL transaction](/docs/transaction), make the desired changes with Postgres acting as your IDE, automatically blocking many impossible changes via types and relations, and use [`iasql_create_review`](/docs/review/) to create a review artifact for your team *after* IaSQL has already vetted it for technical correctness. This vetting makes the result superior to the traditional peer review system in IaC.\\n\\nDatabase schemas can be overwhelming, and a schema accurately representing *all* of AWS would be dizzying. HCL and other IaC tools let you elide parts of AWS via a module system, so we have implemented our own [module system](/docs/module) on top of Postgres inspired equally by programming language module systems and Linux distro package managers. This module system takes the place the usual database [schema migration system](https://en.wikipedia.org/wiki/Schema_migration) for the management of the tables in the database, which may feel unusual at first, but we believe it is superior (for this use-case).\\n\\nEven with IaC tools letting you ignore details you don\'t specifically need, AWS and other clouds expose many details in multiple layers for you to control when often you just need the same subset of functionality over-and-over, so these IaC tools have higher-level modules for these cookie-cutter use-cases. For IaSQL we [did the same](/docs/low-level-vs-high-level), though in a way that coexists with the lower-level modules so you can still query those details later, if you like. [Here](/blog/ecs-simplified) is an example of a high-level module that greatly simplifies ECS fargate. \\n\\nIaSQL is declarative like IaC, so the leaky abstraction problem of declarative programming can still impact things. Like IaC\'s `apply` concept IaSQL has [its own transaction system](/docs/transaction) (separate from the actual [database transaction system](https://www.postgresql.org/docs/current/tutorial-transactions.html)) that will let you enqueue changes and preview what actions the engine will take before commiting to it (or rolling it back). Going beyond that and acknowledging that sometimes you need to directly work in the lower abstraction layer, IaSQL exposes optional [raw SDK access](https://iasql.com/docs/modules/aws/aws_sdk/) within the database to handle those tougher situations, and can then re-synchronize the declarative state afterwards.\\n\\nAs IaSQL is able to both push changes from the database to the cloud and pull changes from the cloud to the database versus the one-way push from HCL that Terraform provides, you can make changes directly in the AWS console, then inspect the audit log directly or [call a special formatting function](https://iasql.com/docs/modules/builtin/tables/iasql_functions_rpcs_iasql_get_sql_since.IasqlGetSqlSince/) to see what SQL statements were the equivalent of those console changes. This reduces the learning curve of IaSQL and also lowers the barrier to entry in comparison to IaC. You can continue using other cloud management tooling in conjunction with IaSQL (including to a degree IaC) and IaSQL will show you \\"it\'s way\\" of doing the same when it syncs.\\n\\nFinally, IaSQL makes your infrastructure *accessible* to a much large portion of your company than IaC tools can. Data Scientists who know SQL could help with the provisioning of their own batch processing code, EngSec can query *all* company infrastructure for security vulnerabilities and propose changes to improve things, Business Analysts in Growth teams can query actual traffic information from the live infrastructure (with read-only AWS credentials, of course), Finance auditors can query your infrastructure joined on pricing data and figure out *accurate* expense reporting per division, and DevOps can run queries joined on CloudWatch utilization stats to automatically identify overprovisioned services and create recommended auto-load scaling configuration changes. Tons of \\"not-developer\\" and \\"developer-adjacent\\" roles that can take over a lot of the ancillary burden of maintaining a production system.\\n\\nBut IaSQL is much newer than the IaC tools. We have coverage at the time of writing for 25 different AWS services, but that is admittedly far from complete. IaC tools also have cloud resources they cannot represent, but it is a much smaller percentage. This disadvantage will diminish over time, but creating a bijective entity representation of every cloud service requires more developer firepower than the approach IaC tools have taken, so we expect to always lag temporally on this front. At least, as long as AWS and friends don\'t follow a resource-oriented standard like [Swagger](https://swagger.io/specification/).\\n\\n:::note Cloud APIs on Swagger?\\n\\n![swagger-i-would-be-so-happy](/img/why-sql-for-infrastructure/swagger-i-would-be-so-happy.jpg)\\n\\n*Me too, Craig, me too...*\\n\\n:::\\n\\nWith that said, what does our EC2 instance example look like in IaSQL, uh, SQL?\\n\\n```sql\\n-- Assuming that the AWS credentials have already been inserted as done during the setup wizard\\nSELECT default_aws_region(\'us-west-2\');\\n\\n-- Install the necessary modules\\nSELECT iasql_install(\'aws_ec2\', \'aws_ec2_metadata\', \'ssh_accounts\');\\n\\n-- Create the key-pair and put it in our ssh_credentials table\\n-- We will update this later with the public IP address\\nINSERT INTO ssh_credentials (\\"name\\", hostname, username, private_key)\\nVALUES (\'login\', \'tbd\', \'ubuntu\', (SELECT privateKey FROM key_pair_request(\'login\', \'us-west-2\')));\\n\\n-- Start an IaSQL transaction so these changes don\'t mutate our infrastructure immediately\\nSELECT iasql_begin();\\n\\n-- Define the security group\\nINSERT INTO security_group (description, group_name)\\nVALUES (\'Login security group\', \'login-sg\');\\n\\n-- And it\'s ingress rule\\nINSERT INTO security_group_rule (is_egress, ip_protocol, from_port, to_port, cidr_ipv4, security_group_id)\\nSELECT false, \'tcp\', 22, 22, \'0.0.0.0/0\', id\\nFROM security_group\\nWHERE group_name = \'login-sg\';\\n\\n-- Define the instance and the key-pair to use\\nINSERT INTO instance (ami, instance_type, key_pair_name, tags, subnet_id)\\n  SELECT\\n    \'resolve:ssm:/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id\',\\n    \'t3.small\',\\n    \'login\',\\n    \'{\\"name\\":\\"login\\"}\',\\n    id\\n  FROM subnet\\n  WHERE availability_zone = \'us-west-2a\'\\n  LIMIT 1;\\n\\n-- Associate the instance with the security group\\nINSERT INTO instance_security_groups (instance_id, security_group_id) SELECT\\n  (SELECT id FROM instance WHERE tags ->> \'name\' = \'login\'),\\n  (SELECT id FROM security_group WHERE group_name=\'login-sg\' AND region = \'us-west-2\');\\n```\\n\\nWe can then `SELECT * FROM iasql_preview();` to see what this does, like a `terraform plan`:\\n\\n:::note `iasql_preview` output\\n| action | table_name | id | description |\\n| :----- | :--------- | -: | ----------: |\\n| create | instance | 1 | 1 |\\n| create | security_group | 18 | 18 |\\n| create | security_group_rule | 1 | 1 |\\n:::\\n\\nThis is a \\"high level\\" overview with minimal details. We can get back a listing of all changes, represented in auto-generated SQL with `SELECT * FROM iasql_get_sql_for_transaction();`\\n\\n```sql\\nINSERT INTO security_group (description, group_name, region)\\nVALUES (\'Login security group\', \'login-sg\', (SELECT region FROM aws_regions WHERE region = \'us-west-2\'));\\n\\n\\nINSERT INTO security_group_rule (is_egress, ip_protocol, from_port, to_port, cidr_ipv4, region, security_group_id)\\nVALUES (\'f\', \'tcp\', \'22\', \'22\', \'0.0.0.0/0\', (SELECT region FROM aws_regions WHERE region = \'us-west-2\'), (SELECT id FROM security_group WHERE group_id = NULL AND region = (SELECT region FROM aws_regions WHERE region = \'us-west-2\')));\\n\\n\\nINSERT INTO instance (ami, instance_type, key_pair_name, state, tags, hibernation_enabled, region, subnet_id)\\nVALUES (\'resolve:ssm:/aws/service/canonical/ubuntu/server/20.04/stable/current/amd64/hvm/ebs-gp2/ami-id\', \'t3.small\', \'login\', \'running\', \'{\\"name\\":\\"login\\"}\'::jsonb, \'f\', (SELECT region FROM aws_regions WHERE region = \'us-west-2\'), (SELECT id FROM subnet WHERE subnet_id = \'subnet-06140fd708a495450\' AND region = (SELECT region FROM aws_regions WHERE region = \'us-west-2\')));\\n\\n\\nINSERT INTO instance_security_groups (instance_id, security_group_id)\\nVALUES ((SELECT id FROM instance WHERE instance_id = NULL AND region = (SELECT region FROM aws_regions WHERE region = \'us-west-2\')), (SELECT id FROM security_group WHERE group_id = NULL AND region = (SELECT region FROM aws_regions WHERE region = \'us-west-2\')));\\n```\\n\\nwhich in this case is very similar to (though a bit more verbose than) our original SQL statements, but if you were going back and forth on what exact changes you\'re going to make, it can be a good summary.\\n\\nIf we\'re good with this, we can `SELECT iasql_commit()` to push this to our cloud account and then update our `ssh_credentials` record with the public IP address and check the connection:\\n\\n```sql\\nWITH h AS (\\n  SELECT host(im.public_ip_address) AS hostname\\n  FROM instance_metadata im\\n  INNER JOIN instance i ON im.instance_id = i.instance_id\\n  WHERE i.tags ->> \'name\' = \'login\'\\n  LIMIT 1\\n)\\nUPDATE ssh_credentials SET hostname = h.hostname FROM ssh_credentials, h WHERE hostname = \'tbd\';\\n\\nSELECT * FROM ssh_exec(\'login\', \'uname -a\');\\n```\\n\\n## SQL is the least bad option for Infrastructure Management\\n\\nInfrastructure Management is a complex beast. The cloud APIs are massive with a rich collection of knobs to tweak, and mistakes can be dangerous because the changes often take a long time and recovery even more so. We have learned about the two main types of infrastructure management: imperative and declarative, where ad-hoc usage of the AWS console fits under the imperative umbrella along with scripts calling an SDK, while IaC has until recently been the only way to declaratively manage your infrastructure.\\n\\nBoth imperative and declarative styles have their downsides. Imperative being more explicit makes it more tedious and more prone to error, and the usual execution frequency of *only once* for most production changes makes scripting usually no better than clicking around in the AWS console. Declarative is much less tedious, but because it is a leaky abstraction inspecting its execution plan is critical to make sure it doesn\'t automatically put you into an outage, and failures during application can still require falling back to imperative mode to get things back into shape from time-to-time.\\n\\nAlso until recently, all declarative approaches have suffered from only being a one-way transition, from your IaC codebase into the cloud, but no good or easy way to synchronize that code with the current state of the cloud if they drift from each other. Because these declarative programming approaches do not have a singular canonical form, they can\'t be bijective, making the transform back from the other side impossible to square up.\\n\\nData, on the other hand, is much easier to normalize in such a way, and relational data models, as found in SQL databases, can naturally model the relationships between cloud services, making it possible to produce a bijective representation. IaSQL takes advantage of this to make a declarative model based on tables and foreign keys that can automatically handle changes done outside of this declarative model, either by dropping down into its own imperative mode or by using any other tooling to manage your infrastructure, giving it a flexibility and resiliency that IaC tools don\'t have.\\n\\nOn top of this foundation, tooling to replicate or substitute for existing IaC tooling has been added, along with all of the past ~50 years of database tooling and expertise that you can take advantage of, providing a superset of functionality that makes existing use-cases easier and new use-cases possible.\\n\\nIaSQL is [currently in beta](/blog/beta) and can be used locally with [`docker`](https://hub.docker.com/r/iasql/iasql). We\'re proud of what we have built, and can\'t wait to hear from you on feature requests and bug reports."},{"id":"deploy-stable-diffusion","metadata":{"permalink":"/blog/deploy-stable-diffusion","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/deploy-stable-diffusion.mdx","source":"@site/blog/tutorials/deploy-stable-diffusion.mdx","title":"Deploy Stable Diffusion in EC2 using a SQL query","description":"Stable Diffusion is a deep learning, text-to-image model initially released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.","date":"2023-03-29T00:00:00.000Z","formattedDate":"March 29, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":5.84,"hasTruncateMarker":true,"authors":[{"name":"Yolanda Robla","imageURL":"https://github.com/yrobla.png","key":"yrobla"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"deploy-stable-diffusion","title":"Deploy Stable Diffusion in EC2 using a SQL query","date":"2023-03-29T00:00:00.000Z","authors":["yrobla","depombo"],"image":"/img/deploy-stable-diffusion/stable-diffusion-screenshot.png","tags":["tutorial"]},"prevItem":{"title":"Why SQL is right for Infrastructure Management","permalink":"/blog/why-sql-for-infrastructure"},"nextItem":{"title":"Save $ on public S3 buckets using VPC endpoints via SQL","permalink":"/blog/save-s3-vpc"}},"content":"Stable Diffusion is a deep learning, text-to-image model initially released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.\\n\\nThere is a publicly available [tool](https://beta.dreamstudio.ai/dream) limited to a maximum of 200 images generated. But because the model is open source, you can download and host your own version of it. There are different approaches to host it, but one easy option is to host it in an EC2 GPU instance, using IaSQL to setup everything with just a few SQL queries. IaSQL is an [open-source](https://github.com/iasql/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pre-requisites\\n\\nStable Diffusion will need to run on GPU instances. These are likely not enabled in your AWS account by default, so you may need to increase your quota. This post uses _p_ instances, so you need to check your [AWS quota](https://us-east-2.console.aws.amazon.com/servicequotas/home/services/ec2/quotas) for the region where you want to deploy your model and check if you have quota for this type. The model needs 8 vCPUs per instance, so please request 8 vCPUS per each instance you want to deploy. As instances will take some time to terminate while other is started, it is recommended to request quota for at least 2 instances.\\n\\n<img src=\\"/img/deploy-stable-diffusion/request_quota_increase.png\\" style={{ maxWidth: 600 }} />\\n\\nThe model needs to integrate with Nvidia GPU to run. This needs to have a custom OS image with drivers installed which does not come built-in. Setting this up is not trivial, but some of the AMIs in the marketplace can help us with that. This post uses an image from the marketplace, so a subscription will be needed to use it.\\nPlease visit the [AWS marketplace](https://aws.amazon.com/marketplace/pp/prodview-64e4rx3h733ru?sr=0-1&ref_=beagle&applicationId=AWS-Marketplace-Console) and subscribe to the Amazon Linux 2 AMI with NVIDIA TESLA GPU Driver AMI offer.\\n\\n<img src=\\"/img/deploy-stable-diffusion/subscribe_to_marketplace_ami.png\\" style={{ maxWidth: 600 }} />\\n\\n**Please note that you will incur a cost of $3.06/Hour when using this AMI**\\n\\n## Stable Diffusion Installation\\n\\nWhile Stable Diffusion is a complex model with lots of dependencies, there are pre-built Docker images from the community that ships with all the components in place. We will rely on [Sygil WebUI](https://sygil-dev.github.io/sygil-webui/docs/Installation/docker-guide) images to deploy our model. These images are ready to be used out of the box, but there is the possibility to build and customize the images in case it\'s needed.\\n\\nWhile this deployment may seem complex at first glance, it can be made easy with [IaSQL](https://iasql.com/docs). In the following section will take you through the steps needed to deploy Stable Diffusion just using SQL queries to inspect and modify the AWS infrastructure.\\n\\nWe would need to start by getting the AMI ID for the image on the marketplace, to boot images based on that. We can get it with a simple query using IaSQL SDK module:\\n\\n```sql title=\\"Get AMI ID\\"\\n -- install modules\\nSELECT\\n  *\\nFROM\\n  iasql_install (\'aws_sdk\');\\n\\n-- query for the image based on the image description\\nSELECT\\n  invoke_ec2 (\\n    \'describeImages\',\\n    \'{\\"Filters\\": [{\\"Name\\": \\"description\\", \\"Values\\": [\\"Amazon Linux 2 Graphics AMI 2.0.20230119.1 x86_64 HVM gp2\\"]}, {\\"Name\\": \\"architecture\\", \\"Values\\": [\\"x86_64\\"]}]}\'\\n  ) -> \'Images\' -> -1 -> \'ImageId\';\\n```\\n\\nNow with all prerequisites solved, deploying Stable Diffusion can be done with one click using IaSQL engine. Please note that we will be using a beef p3.2xlarge instance with extra disk space storage, so running this experiment will incur some extra costs:\\n\\n```sql title=\\"Deploy Stable Diffusion\\"\\n -- install modules\\nSELECT\\n  *\\nFROM\\n  iasql_install (\'aws_ec2\', \'aws_ec2_metadata\');\\n\\n-- start transaction\\nSELECT\\n  *\\nFROM\\n  iasql_begin ();\\n\\n-- set us-east-2 region as default to use it for all the resources in the deployment\\nUPDATE\\n  aws_regions\\nSET\\n  is_default = FALSE;\\n\\nUPDATE\\n  aws_regions\\nSET\\n  is_default = TRUE\\nWHERE\\n  region = \'us-east-2\';\\n\\n-- insert security groups\\nINSERT INTO\\n  security_group (description, group_name)\\nVALUES\\n  (\'Stable Diffusion test security group\', \'stable_diffusion_sg\');\\n\\nINSERT INTO\\n  security_group_rule (is_egress, ip_protocol, from_port, to_port, cidr_ipv4, description, security_group_id)\\nSELECT\\n  t.is_egress,\\n  t.ip_protocol,\\n  t.from_port,\\n  t.to_port,\\n  t.cidr_ipv4::cidr,\\n  t.description,\\n  security_group.id\\nFROM\\n  security_group,\\n  (\\n    VALUES\\n      (FALSE, \'tcp\', 22, 22, \'0.0.0.0/0\', \'stable_diffusion_sg_rule_ssh\'),\\n      (FALSE, \'tcp\', 80, 80, \'0.0.0.0/0\', \'stable_diffusion_sg_rule_http\'),\\n      (TRUE, \'tcp\', 1, 65535, \'0.0.0.0/0\', \'stable_diffusion_sg_rule_egress\')\\n  ) AS t (is_egress, ip_protocol, from_port, to_port, cidr_ipv4, description)\\nWHERE\\n  security_group.group_name = \'stable_diffusion_sg\';\\n\\n-- insert instance with the right userdata\\nINSERT INTO\\n  general_purpose_volume (size, volume_type, availability_zone, tags, is_root_device)\\nVALUES\\n  (60, \'gp2\', \'us-east-2a\', \'{\\"name\\": \\"stable-diffusion-blog\\"}\', TRUE);\\n\\nINSERT INTO\\n  instance (ami, instance_type, subnet_id, tags, user_data, region)\\nSELECT\\n  \'ami-0890479ea4c515bf8\', -- this AMI ID is the one we got from the query above\\n  \'p3.2xlarge\',\\n  subnet.id,\\n  \'{\\"name\\":\\"stable-diffusion-blog\\"}\',\\n  \'#!/bin/bash\\nyum update -y\\n\\n# install nginx\\namazon-linux-extras install nginx1 -y \\nsystemctl enable nginx\\nsystemctl start nginx\\n\\ncat > /etc/nginx/conf.d/stable_diffusion.conf << EOF\\nserver {\\n    listen 80;\\n    index index.php index.html index.htm;\\n    location / {\\n        proxy_pass http://0.0.0.0:8501/;\\n        proxy_set_header X-Forwarded-For \\\\$proxy_add_x_forwarded_for;\\n        proxy_set_header Host \\\\$http_host;\\n        proxy_redirect off;\\n        proxy_http_version 1.1;\\n        proxy_set_header Upgrade \\\\$http_upgrade;\\n        proxy_set_header Connection \\"upgrade\\";\\n    }\\n    error_page 404 /404.html;\\n    error_page 500 502 503 504 /50x.html;\\n\\n    location = /50x.html {\\n        root /usr/share/nginx/html;\\n    }\\n}\\nEOF\\n\\n# install docker\\nyum -y install docker\\nusermod -a -G docker ec2-user\\nid ec2-user\\nnewgrp docker\\n\\nsystemctl enable docker.service\\nsystemctl start docker.service\\n\\n# install nvidia docker toolkit\\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)    && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\\nyum -y clean  expire-cache\\npushd /tmp\\nwget https://github.com/NVIDIA/libnvidia-container/raw/gh-pages/stable/amzn2/x86_64/libnvidia-container-tools-1.12.0-1.x86_64.rpm\\nyum -y install  libnvidia-container-tools-1.12.0-1.x86_64.rpm\\nyum install -y nvidia-container-toolkit\\nnvidia-ctk runtime configure --runtime=docker\\nsystemctl restart docker\\npopd\\n\\n# run docker image for stable diffusion\\ndocker pull hlky/sd-webui:runpod\\ndocker container run --rm -d -p 8501:8501 -e STREAMLIT_SERVER_HEADLESS=true -e \\"WEBUI_SCRIPT=webui_streamlit.py\\" --runtime=nvidia  -e \\"VALIDATE_MODELS=false\\" -v \\"${PWD}/outputs:/sd/outputs\\" --gpus all hlky/sd-webui:runpod\\nsystemctl reload nginx\\n\',\\n  \'us-east-2\'\\nFROM\\n  subnet\\n  INNER JOIN vpc ON vpc.id = subnet.vpc_id\\n  AND vpc.is_default = \'Y\'\\nWHERE\\n  vpc.region = \'us-east-2\'\\n  AND subnet.availability_zone = \'us-east-2a\'\\nLIMIT\\n  1;\\n\\nINSERT INTO\\n  instance_block_device_mapping (device_name, volume_id, instance_id)\\nVALUES\\n  (\\n    \'/dev/xvda\',\\n    (\\n      SELECT\\n        id\\n      FROM\\n        general_purpose_volume\\n      WHERE\\n        tags ->> \'name\' = \'stable-diffusion-blog\'\\n      LIMIT\\n        1\\n    ),\\n    (\\n      SELECT\\n        id\\n      FROM\\n        instance\\n      WHERE\\n        tags ->> \'name\' = \'stable-diffusion-blog\'\\n      LIMIT\\n        1\\n    )\\n  );\\n\\n-- insert instance security groups\\nINSERT INTO\\n  instance_security_groups (instance_id, security_group_id)\\nSELECT\\n  (\\n    SELECT\\n      id\\n    FROM\\n      instance\\n    WHERE\\n      tags ->> \'name\' = \'stable-diffusion-blog\'\\n    LIMIT\\n      1\\n  ),\\n  (\\n    SELECT\\n      id\\n    FROM\\n      security_group\\n    WHERE\\n      group_name = \'stable_diffusion_sg\'\\n      AND region = \'us-east-2\'\\n    LIMIT\\n      1\\n  );\\n\\nSELECT\\n  *\\nFROM\\n  iasql_commit ();\\n```\\n\\n## Accessing Stable Diffusion\\n\\nThe installation procedure will run to completion without intervention and will take over 10-15 minutes, due to the volume of the artifacts to install. After the process finishes you will have your Stable Diffusion web interface up and running, publicly accessible through the public IP of the recently created instance. You can get the details of the public IP of your instance by querying the metadata table in IaSQL:\\n\\n```sql\\nSELECT\\n  public_ip_address\\nFROM\\n  instance_metadata\\nWHERE\\n  id = (\\n    SELECT\\n      id\\n    FROM\\n      instance\\n    WHERE\\n      tags ->> \'name\' = \'stable-diffusion-blog\'\\n    LIMIT\\n      1\\n  );\\n```\\n\\nThe initial Stable Diffusion installation will be available on the browser just query by `http://<public_ip_address>/`:\\n\\n<img src=\\"/img/deploy-stable-diffusion/stable-diffusion-screenshot.png\\" style={{ maxWidth: 600 }} />\\n\\nTime to enjoy your image generation and start being creative with Stable Diffusion!"},{"id":"save-s3-vpc","metadata":{"permalink":"/blog/save-s3-vpc","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/optimize-s3-endpoints.mdx","source":"@site/blog/tutorials/optimize-s3-endpoints.mdx","title":"Save $ on public S3 buckets using VPC endpoints via SQL","description":"Are you using S3 buckets as part of your cloud deployments? How are you accessing them?","date":"2023-03-06T00:00:00.000Z","formattedDate":"March 6, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":6.76,"hasTruncateMarker":true,"authors":[{"name":"Yolanda Robla","imageURL":"https://github.com/yrobla.png","key":"yrobla"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"save-s3-vpc","title":"Save $ on public S3 buckets using VPC endpoints via SQL","authors":["yrobla","depombo"],"date":"2023-03-06T00:00:00.000Z","tags":["tutorial"]},"prevItem":{"title":"Deploy Stable Diffusion in EC2 using a SQL query","permalink":"/blog/deploy-stable-diffusion"},"nextItem":{"title":"Securely connect to an Amazon RDS via PrivateLink using SQL","permalink":"/blog/rds-privatelink"}},"content":"Are you using S3 buckets as part of your cloud deployments? How are you accessing them?\\n\\nWhen running applications behind VPCs without public access, there may be the need to access S3 buckets from the private subnet over the public internet.\\nOne simple but costly way to do so is to rely on [NAT gateways](https://docs.aws.amazon.com/en_en/vpc/latest/userguide/vpc-nat-gateway.html).\\n\\n<img src=\\"/img/optimize-s3-endpoints/s3_nat_gateway.png\\" style={{ maxWidth: 400 }} />\\n\\nHowever, creating [gateway or interface VPC endpoints](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html) for each region where your buckets are exposed is a more optimal solution.\\n\\n<img src=\\"/img/optimize-s3-endpoints/s3_privatelink.png\\" style={{ maxWidth: 600 }} />\\n\\nWhen the VPC endpoints are enabled you can access your S3 buckets using this endpoint. In this post, we will walk you through how to control your buckets from an internal network with the desired security, and without the extra costs of a NAT gateway using a VPC endpoint and IaSQL. IaSQL is an [open-source](https://github.com/iasql/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.\\n\\n\x3c!-- truncate --\x3e\\n\\n### Why use VPC Interface Endpoints\\n\\nAWS VPC Interface Endpoints are a component of AWS\'s [PrivateLink](https://aws.amazon.com/privatelink) infrastructure. AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet.\\n\\nRelying on PrivateLink offers a set of advantages in terms of **cost**, **security** and **performance**.\\n\\n**Cost**\\n\\nWhen using NAT gateways, you are paying for the NAT gateway itself, and the bandwidth used to access the internet. A NAT gateway is a generic way for private instances to reach the internet, and can be useful for generic usage, such as pulling dependencies, updating repositories, etc... But when used for reaching AWS resources directly, there are more optimal and cheaper solutions such as VPC endpoints - they will not have generic access to internet but only to specified public AWS endpoints. When using VPC endpoints, you are paying for the bandwidth used to access the public endpoints, but not for the VPC endpoint itself.\\n\\nCosts for NAT gateway depend on each region, but it should be over _0.045 USD/hour_ per GB processed + _0.045 USD/hour_ for the instance itself. Costs for VPC endpoints are charged at _0.01 USD/hour_ per GB processed, being cheaper after the first PB.\\n\\n| Component    | Hourly pricing | Per GB pricing       |\\n| ------------ | -------------- | -------------------- |\\n| NAT gateway  | 0.045 USD/hour | 0.45 USD/hour per GB |\\n| VPC endpoint | --             | 0.01 USD/hour per GB |\\n\\n**Security: no internet traversal**\\n\\nWhen deploying services in production, the concept of defense-in-depth is essential: have an information assurance strategy that provides multiple, redundant defensive measures in case a security control fails or a vulnerability is exploited.\\nBy using interface endpoints the services will be accessed via Amazon\'s internal networks. It is an essential security measure, that will prevent attackers from directly reaching the services by keeping them in a private network, reducing the surface area of attack.\\n\\n**Security: policies**\\n\\nAWS services can benefit from [IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_controlling.html) for fine-grained access control. When using interface endpoints, those policies can be applied to determine the traffic that can pass through the interface.\\n\\n**Performance: latency**\\n\\nBecause the traffic stays within Amazon\u2019s networks, the physical distance traveled, the number of hops and the risk of traversing congested networks are all significantly lower.\\n\\n**Performance: bandwidth**\\n\\nPrivateLink supports a sustained bandwidth of 10 Gbps per availability zone with bursts up to 40 Gbps.\\n\\n**Performance: stability**\\n\\nPrivateLink consistently lowers the number of errors and timeouts on high loads. The distance traveled, the number of hops, and the risks associated with traversing congested networks are reduced when the traffic over Private Link stays within Amazon\u2019s networks.\\n\\nIn terms of speed, Private Link supports a sustained bandwidth of 10 Gbps per availability zone with bursts up to 40 Gbps.\\n\\n**Do you want to know if you have it properly configured? The following query will check for active S3 VPC interface endpoints:**\\n\\nThis query will find your active S3 buckets for all regions, associated with the existing endpoint gateways or interfaces - if they exist.\\n\\n```sql title=\\"Query for missing endpoints\\"\\n -- Installing the needed modules\\nSELECT\\n  iasql_install (\'aws_s3\', \'aws_vpc\');\\n\\n-- Perform the query for endpoints\\nSELECT\\n  bucket.region,\\n  vpc.is_default,\\n  vpc.cidr_block,\\n  (\\n    SELECT\\n      COUNT(*) > 0\\n    FROM\\n      endpoint_gateway\\n    WHERE\\n      endpoint_gateway.region = bucket.region\\n      AND service = \'s3\'\\n      AND endpoint_gateway.vpc_id = vpc.id\\n  ) AS has_endpoint_gateway,\\n  (\\n    SELECT\\n      COUNT(*) > 0\\n    FROM\\n      endpoint_interface\\n    WHERE\\n      endpoint_interface.region = bucket.region\\n      AND service = \'s3\'\\n      AND endpoint_interface.vpc_id = vpc.id\\n  ) AS has_endpoint_interface\\nFROM\\n  bucket\\n  LEFT OUTER JOIN vpc ON vpc.region = bucket.region;\\n```\\n\\nHave you found missing endpoints? No problem, IaSQL can generate them for you. You can create two different types of endpoints: gateway and interface. We\'re going to describe their main features and you can create the desired type in an automated way.\\n\\n## Interface endpoints\\n\\nAn interface endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined for a supported AWS service. As they use ENIs, they can benefit from security groups to control traffic.\\n\\n[IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_controlling.html) can also be\\nadded to control access to those endpoints.\\n\\nThey are regional, meaning they can only be accessed by the same region they are created. However, Multi-region is possible by also using VPC peering, so resources in one region can be accessed from others, though this only supports IPv4 TCP traffic.\\n\\nAn interface endpoint (except S3 interface endpoint) has a corresponding private DNS hostname, that can be used to access the resource.\\n\\nInterface endpoints cover a wide range of services such as S3, Lambda, API Gateway, etc... [Get more detail about interface endpoints](https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html)\\n\\nThis query will auto-generate the missing interface endpoints and will preview the changes to be applied on your cloud. Simply uncomment the final statement and run it to make the changes to your cloud account.\\n\\n```sql title=\\"Insert missing endpoints\\"\\n -- Inserts the missing endpoints\\nSELECT\\n  *\\nFROM\\n  iasql_begin ();\\n\\nINSERT INTO\\n  endpoint_interface (region, vpc_id, service)\\nSELECT\\n  bucket.region,\\n  vpc.id,\\n  \'s3\'\\nFROM\\n  bucket\\n  INNER JOIN vpc ON bucket.region = vpc.region\\nWHERE\\n  NOT EXISTS (\\n    SELECT\\n      id\\n    FROM\\n      endpoint_interface\\n    WHERE\\n      endpoint_interface.region = bucket.region\\n      AND endpoint_interface.vpc_id = vpc.id\\n  );\\n\\n-- Preview the changes\\nSELECT\\n  *\\nFROM\\n  iasql_preview ();\\n\\n-- Commit the changes\\n-- SELECT * FROM iasql_commit();\\n-- Rollback changes\\n-- SELECT * FROM iasql_rollback();\\n```\\n\\n## Endpoint gateways\\n\\nAn endpoint gateway is a gateway that can be configured as a target for a route in your route table, used to access traffic in DynamoDB or S3.\\n\\nMultiple gateway endpoints can be created in a single VPC, and those can be used on different route tables to enforce different access policies from different subnets to the same service.\\n\\nGateway endpoints are supported within the same region only, resources from multiple regions cannot be accessed, even using VPC peering. They also support IPv4 traffic only.\\n\\nTo use them, DNS resolution must be enabled in the VPC.\\n\\nWhen a route is added, all instances in the subnets associated with the route table will automatically use the endpoint to access the service.\\n\\nGateway endpoints only support S3 and DynamoDB services. [Get more detail about gateway endpoints](https://docs.aws.amazon.com/vpc/latest/privatelink/gateway-endpoints.html).\\n\\nThis query will auto-generate the missing endpoint gateways and will allow you to preview the changes to be applied on your cloud.\\n\\n```sql title=\\"Insert missing Gateway endpoints\\"\\n -- Inserts the missing endpoints\\nSELECT\\n  *\\nFROM\\n  iasql_begin ();\\n\\nINSERT INTO\\n  endpoint_gateway (region, vpc_id, service)\\nSELECT\\n  bucket.region,\\n  vpc.id,\\n  \'s3\'\\nFROM\\n  bucket\\n  INNER JOIN vpc ON bucket.region = vpc.region\\nWHERE\\n  NOT EXISTS (\\n    SELECT\\n      id\\n    FROM\\n      endpoint_gateway\\n    WHERE\\n      endpoint_gateway.region = bucket.region\\n      AND endpoint_gateway.vpc_id = vpc.id\\n  )\\n  -- Preview the changes\\nSELECT\\n  *\\nFROM\\n  iasql_preview ();\\n\\n-- Commit the changes\\n-- SELECT * FROM iasql_commit();\\n-- Rollback changes\\n-- SELECT * FROM iasql_rollback();\\n```\\n\\n## Testing the result\\n\\nOnce all the relevant endpoints have been created, you can confirm your access to the gateway endpoints from an internal EC2 instance on the same region as the endpoint you want to test:\\n\\n<img src=\\"/img/optimize-s3-endpoints/ec2_using_vpc.png\\" style={{ maxWidth: 500 }} />\\n\\nFor interface endpoints, access also can be tested using the named endpoint:\\n\\n<img src=\\"/img/optimize-s3-endpoints/s3_via_interface.png\\" />"},{"id":"rds-privatelink","metadata":{"permalink":"/blog/rds-privatelink","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/secure-rds-endpoints.mdx","source":"@site/blog/tutorials/secure-rds-endpoints.mdx","title":"Securely connect to an Amazon RDS via PrivateLink using SQL","description":"Do you have some database instances on RDS and wonder what\'s the most secure way to reach them? In this post, we will walk you through how to securely connect to an AWS RDS instance using PrivateLink and IaSQL. AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. IaSQL is an open-source software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.","date":"2023-03-06T00:00:00.000Z","formattedDate":"March 6, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":2.75,"hasTruncateMarker":true,"authors":[{"name":"Yolanda Robla","imageURL":"https://github.com/yrobla.png","key":"yrobla"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"rds-privatelink","title":"Securely connect to an Amazon RDS via PrivateLink using SQL","authors":["yrobla","depombo"],"date":"2023-03-06T00:00:00.000Z","tags":["tutorial"]},"prevItem":{"title":"Save $ on public S3 buckets using VPC endpoints via SQL","permalink":"/blog/save-s3-vpc"},"nextItem":{"title":"Deploy a Static Website on AWS using SQL","permalink":"/blog/deploy-static-website"}},"content":"Do you have some database instances on RDS and wonder what\'s the most secure way to reach them? In this post, we will walk you through how to securely connect to an AWS RDS instance using PrivateLink and IaSQL. AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. IaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen creating a database, AWS provides some specific information about the hostnames and ports used. You can access those to reach your databases by using specific clients for MySQL, Postgres, etc...:\\n\\n<img src=\\"/secure-rds/how_to_connect_rds.png\\" />\\n\\nOn a public VPC, those endpoints will be public by default. However, consider creating them under a private VPC and accessing them internally, to grant additional security to critical services. AWS PrivateLink offers the possibility to securely access those services without exposing them to the internet, just using Amazon\'s private network.\\n\\n### Are my RDS instances properly configured?\\n\\nPlease use this query to check it:\\n\\n```sql title=\\"Run SQL to check your endpoints\\"\\n ---- Installing the needed modules\\nSELECT\\n  iasql_install (\'aws_rds\', \'aws_vpc\');\\n\\n-- Perform the query for endpoints\\nSELECT\\n  rds.region,\\n  vpc.is_default,\\n  vpc.cidr_block,\\n  (\\n    SELECT\\n      COUNT(*) > 0\\n    FROM\\n      endpoint_interface\\n    WHERE\\n      endpoint_interface.region = rds.region\\n      AND service = \'rds\'\\n      AND endpoint_interface.vpc_id = vpc.id\\n  ) AS has_endpoint_interface\\nFROM\\n  rds\\n  LEFT OUTER JOIN vpc ON vpc.region = rds.region;\\n```\\n\\nHave you found missing endpoints? No problem, IaSQL can generate missing Endpoint Interfaces for you:\\n\\n```sql title=\\"Add missing endpoint interfaces\\"\\nSELECT\\n  *\\nFROM\\n  iasql_begin ();\\n\\n-- Inserts the missing endpoints\\nINSERT INTO\\n  endpoint_interface (region, vpc_id, service, private_dns_enabled)\\nSELECT\\n  RDS.region,\\n  vpc.id,\\n  \'rds\',\\n  TRUE\\nFROM\\n  rds\\n  INNER JOIN vpc ON rds.region = vpc.region\\nWHERE\\n  NOT EXISTS (\\n    SELECT\\n      id\\n    FROM\\n      endpoint_interface\\n    WHERE\\n      endpoint_interface.region = rds.region\\n      AND endpoint_interface.vpc_id = vpc.id\\n  );\\n\\n-- Preview the changes\\nSELECT\\n  *\\nFROM\\n  iasql_preview ();\\n\\n-- Apply the changes\\n--select * from iasql_commit();\\n-- Rollback the changes\\nselect * from iasql_rollback();\\n```\\n\\nRunning this query on an IaSQL database will auto-generate the missing endpoint interfaces and will allow you to preview the changes to be applied on your cloud. Once you are OK with the results, you can uncomment the `iasql_commit` query, comment the `iasql_rollback` query, and it will create the endpoints for you. If the final results in the cloud are not as expected, you can always roll back your changes by calling the `iasql_rollback` command.\\n\\n### Testing the result\\n\\nAfter running the query, you should have Endpoint Interfaces created for your RDS resources. Those should be on the region and VPC where you had your databases:\\n\\n<div class=\\"col col--12\\">\\n  <img src=\\"/secure-rds/rds_endpoint_interfaces_created.png\\" height=\\"200\\" />\\n</div>\\n\\nTo start testing the result, you can start a new EC2 instance on a private VPC in the same region where your RDS and Endpoint interface is configured. You can double-check that there is no internet connectivity. But the RDS endpoint could still be reached, by using the interface that has been created:\\n\\n<div class=\\"col col--12\\">\\n  <img src=\\"/secure-rds/access_rds_endpoint.png\\" height=\\"200\\" />\\n</div>\\n\\nPlease note that you could only use those endpoints from the same region. You could reach the services in multiple regions with the use of [VPC peering](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)"},{"id":"deploy-static-website","metadata":{"permalink":"/blog/deploy-static-website","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/deploy-static-website.mdx","source":"@site/blog/tutorials/deploy-static-website.mdx","title":"Deploy a Static Website on AWS using SQL","description":"Did you know you can deploy a static website using a SQL REPL? In this post, we\'ll show you how to use IaSQL to deploy a static website from your GitHub repository to AWS S3 + Cloudfront services using only SQL queries. IaSQL is an open-source software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.","date":"2023-03-03T00:00:00.000Z","formattedDate":"March 3, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":6.975,"hasTruncateMarker":true,"authors":[{"name":"Mohammad Teimori Pabandi","imageURL":"https://github.com/mtp1376.png","key":"mtp1376"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"deploy-static-website","title":"Deploy a Static Website on AWS using SQL","date":"2023-03-03T00:00:00.000Z","authors":["mtp1376","depombo"],"tags":["tutorial"]},"prevItem":{"title":"Securely connect to an Amazon RDS via PrivateLink using SQL","permalink":"/blog/rds-privatelink"},"nextItem":{"title":"IaSQL is in beta!","permalink":"/blog/beta"}},"content":"Did you know you can deploy a static website using a SQL REPL? In this post, we\'ll show you how to use IaSQL to deploy a static website from your GitHub repository to AWS S3 + Cloudfront services using only SQL queries. IaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.\\n\\nWe will create and configure an S3 bucket to serve our static website. To enable support for HTTPS, we\'ll also add a CloudFront distribution. We will also leverage CodeBuild to automatically build the files for our project and copy them to the S3 bucket created already.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Create an S3 Bucket\\n\\nTo be able to work with S3, we should first install the corresponding IaSQL module.\\n\\n```sql title=\\"Install S3 Module\\"\\nSELECT\\n  iasql_install (\'aws_s3\');\\n```\\n\\nInstalling a module gives us the ability to use tables and RPCs provided by it. `aws_s3` module gives us the ability to manage an S3 bucket, S3 static website hosting, and other related stuff. So let\'s create an S3 bucket first.\\n\\n```sql title=\\"Create an S3 Bucket\\"\\nSELECT\\n  iasql_begin ();\\n\\nINSERT INTO\\n  bucket (NAME, policy_document, region)\\nVALUES\\n  (\\n    \'<bucket-name>\',\\n    \'{\\n  \\"Version\\": \\"2012-10-17\\",\\n  \\"Statement\\": [\\n    {\\n      \\"Sid\\": \\"PublicReadGetObject\\",\\n      \\"Effect\\": \\"Allow\\",\\n      \\"Principal\\": \\"*\\",\\n      \\"Action\\": [\\n        \\"s3:GetObject\\"\\n      ],\\n      \\"Resource\\": [\\n        \\"arn:aws:s3:::<bucket-name>/*\\"\\n      ]\\n    }\\n  ]\\n}\',\\n    \'us-east-1\'\\n  );\\n\\nSELECT\\n  iasql_commit ();\\n```\\n\\nThe above query will create a new bucket in the `us-east-1` region with the defined name `<bucket-name>` and the given policy using IaSQL. The `iasql_begin` and `iasql_commit` functions are RPCs that will start and finish an IaSQL transaction. Learn more about IaSQL transactions in this part of our [documentation](/docs/transaction/).\\n\\nNow that we have a bucket, we can upload a file to it and see if we\'re able to view it using our web browser. Let\'s use IaSQL to upload a file to our newly created bucket:\\n\\n```sql title=\\"Upload a File to S3 Bucket\\"\\nSELECT\\n  *\\nFROM\\n  s3_upload_object (\'<bucket-name>\', \'hello.txt\', \'Hello IaSQL!\', \'text/plain\');\\n```\\n\\nThis is going to upload a file named `hello.txt` in our bucket whose content is `Hello IaSQL!`.\\n\\n:::note\\nYou can read the code for [`s3_upload_object` RPC](https://github.com/alantech/iasql/blob/c70f068c7520baf00cea9ddd3a76b8c6dbd2b23b/src/modules/aws_s3/rpcs/s3_upload_object.ts#L27-L33) as well as all other IaSQL modules and RPCs in our GitHub [repository](https://github.com/alantech/iasql/) to see how they work.\\n:::\\n\\nLet\'s see if we can access our file using the S3 bucket URL. It should be as follows:\\n\\n```\\nhttps://<bucket-name>.s3.amazonaws.com/hello.txt\\n```\\n\\nBut we\'re unable to access the file directly because S3 blocks public access by default.\\n\\n<img src=\\"/deploy-static-website/s3-access-denied.png\\" />\\n\\n## Make The Bucket Public\\n\\nWe need to enable public access to our bucket files to be able to directly access the files. We can use the `public_access_block` table provided by `aws_s3` module to allow for public requests to reach our objects.\\n\\n:::note\\nIf you want to know which resources (via tables) an IaSQL module handles, you can visit our documentation page. It also provides a list and explanation of all the RPCs that are provided by a module. In our case, we can visit [this link](https://iasql.com/docs/modules/aws/aws_s3/) to get a list of tables and RPCs available for `aws_s3` module.\\n:::\\n\\n```sql title=\\"Enable Public Access to The Bucket\\"\\nSELECT\\n  iasql_begin ();\\n\\nINSERT INTO\\n  public_access_block (bucket_name, block_public_acls, ignore_public_acls, block_public_policy, restrict_public_buckets)\\nVALUES\\n  (\'<bucket-name>\', FALSE, FALSE, FALSE, FALSE) ON CONFLICT (bucket_name)\\nDO\\nUPDATE\\nSET\\n  block_public_acls = excluded.block_public_acls,\\n  ignore_public_acls = excluded.ignore_public_acls,\\n  block_public_policy = excluded.block_public_policy,\\n  restrict_public_buckets = excluded.restrict_public_buckets;\\n\\nSELECT\\n  iasql_commit ();\\n```\\n\\nThere\'s a 1-1 relationship between the bucket and bucket public access block, therefore we\'re using Postgres [`ON CONFLICT`](https://www.postgresql.org/docs/current/sql-insert.html#SQL-ON-CONFLICT) syntax so that when there\'s a record already, we can replace it without hassle.\\n\\nNow we should be able to directly access our file through the web browser.\\n\\n```\\nhttps://<bucket-name>.s3.amazonaws.com/hello.txt\\n```\\n\\n<img src=\\"/deploy-static-website/hello-iasql.png\\" />\\n\\n## Use S3 Static Website Hosting\\n\\nBut simply serving the files doesn\'t mean we can host a static website. We need to enable [static website hosting](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html) for our bucket to be able to deploy a React codebase. So let\'s enable it.\\n\\n```sql title=\\"Enable S3 Static Website Hosting\\"\\nSELECT\\n  iasql_begin ();\\n\\nINSERT INTO\\n  bucket_website (bucket_name, index_document)\\nVALUES\\n  (\'<bucket-name>\', \'index.html\');\\n\\nSELECT\\n  iasql_commit ();\\n```\\n\\nWe\'ll use this functionality to route all the requests to `index.html` file. This way we can deploy a sample React application and serve it through S3. To get the link for our S3 bucket\'s static website, we can use `get_bucket_website_endpoint` function.\\n\\n```sql\\nSELECT\\n  *\\nFROM\\n  get_bucket_website_endpoint (\'<bucket-name>\');\\n```\\n\\n## Build The Project And Sync To S3\\n\\nNow that everything is set, we just need to build our React app and deploy it to S3. We have already pushed a sample app to this repository:\\n\\n```\\nhttps://github.com/iasql/sample-react-app\\n```\\n\\nBut you can use whatever codebase you\'d like by changing the URLs so that they point to the Github repository hosting your React app.\\n\\nNow it\'s time to create a CodeBuild project. CodeBuild is an AWS CI/CD system that is free of cost. The CodeBuild project will do the following:\\n\\n- Pull the codebase from the GitHub repository\\n- Build the app\\n- Copy the resulting files (`build/*`) to the S3 bucket\\n\\nWe can do this with the following SQL query:\\n\\n```sql title=\\"Create a CodeBuild Project to Build and Deploy The Website\\"\\nSELECT\\n  iasql_begin ();\\n\\n-- create the needed role for codebuild\\nINSERT INTO\\n  iam_role (role_name, assume_role_policy_document, attached_policies_arns)\\nVALUES\\n  (\\n    \'deploy-static-website-role\',\\n    \'{\\n    \\"Statement\\": [\\n      {\\n        \\"Effect\\": \\"Allow\\",\\n        \\"Principal\\": {\\n          \\"Service\\": \\"codebuild.amazonaws.com\\"\\n        },\\n        \\"Action\\": \\"sts:AssumeRole\\"\\n      }\\n    ],\\n    \\"Version\\": \\"2012-10-17\\"\\n  }\',\\n    ARRAY[\\n      \'arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess\',\\n      \'arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\',\\n      \'arn:aws:iam::aws:policy/AmazonS3FullAccess\'\\n    ]\\n  );\\n\\n-- create the codebuild project\\nINSERT INTO\\n  codebuild_project (project_name, build_spec, source_type, privileged_mode, service_role_name, region)\\nVALUES\\n  (\\n    \'deploy-static-website\',\\n    \'version: 0.2\\n\\nphases:\\n  pre_build:\\n    commands:\\n      - git clone https://github.com/iasql/sample-react-app && cd sample-react-app\\n  build:\\n    commands:\\n      - echo Installing dependencies\\n      - npm install\\n      - echo Building the app\\n      - npm run build\\n  post_build:\\n    commands:\\n      - echo Copying the files to the S3 bucket\\n      - aws s3 sync build/ s3://<bucket-name>\',\\n    \'NO_SOURCE\',\\n    FALSE,\\n    \'deploy-static-website-role\',\\n    \'us-east-1\'\\n  );\\n\\nSELECT\\n  iasql_commit ();\\n```\\n\\nThe above SQL command first creates a role that is needed for CodeBuild to operate. Then it\'ll create the actual CodeBuild project that clones the repo, builds it and finally syncs the resulting files to our S3 bucket. We need to trigger the CodeBuild project to run and then our files will be uploaded to our bucket.\\n\\n```sql title=\\"Trigger The CodeBuild Project\\"\\nSELECT\\n  *\\nFROM\\n  start_build (\'deploy-static-website\', \'us-east-1\');\\n```\\n\\nThis will trigger the CodeBuild project to start. After a while, we should be able to see the files appearing in our S3 bucket. We can access our React app using the endpoint for the S3 static website hosting. As we already mentioned, to get the endpoint we can use `get_bucket_website_endpoint` helper function.\\n\\n```sql title=\\"Get The Bucket Website Endpoint\\"\\nSELECT\\n  get_bucket_website_endpoint (\'<bucket-name>\');\\n```\\n\\nBy visiting the link returned by the above function, you can see our sample app has been deployed. The problem is that S3 static website hosting does not support HTTPS, and therefore we need to use a CloudFront distribution in order to have HTTPS connection.\\n\\n## Create a CloudFront Distribution\\n\\nServing files in a bucket to the public using pure S3 isn\'t a good idea. In our case because the S3 static website hosting does not provide an HTTPS endpoint, but in most cases because the [data transfer rates for S3](https://aws.amazon.com/s3/pricing/) aren\'t cheap and can grow out of control. Also, the speed at which the users can access the bucket objects will increase if you use a CDN because they\'ll be delivered to the users from the nearest edge server.\\n\\nWith the above in mind, let\'s create a CloudFront distribution for our S3 bucket. But first, we need to install the `aws_cloudfront` module to be able to leverage its abilities.\\n\\n```sql\\nSELECT\\n  iasql_install (\'aws_cloudfront\');\\n```\\n\\nThen create the distribution:\\n\\n```sql title=\\"Create a Distribution For Bucket Website\\"\\nSELECT\\n  iasql_begin ();\\n\\nINSERT INTO\\n  distribution (caller_reference, origins, default_cache_behavior)\\nVALUES\\n  (\\n    \'my-website\',\\n    (\\n      \'[\\n  {\\n    \\"DomainName\\": \\"\' || (\\n        SELECT\\n          get_bucket_website_endpoint (\'<bucket-name>\')\\n      ) || \'\\",\\n    \\"Id\\": \\"my-website-origin\\",\\n    \\"CustomOriginConfig\\": {\\n      \\"HTTPPort\\": 80,\\n      \\"HTTPSPort\\": 443,\\n      \\"OriginProtocolPolicy\\": \\"http-only\\"\\n    }\\n  }\\n]\'\\n    )::json,\\n    \'{\\n  \\"TargetOriginId\\": \\"my-website-origin\\",\\n  \\"ViewerProtocolPolicy\\": \\"allow-all\\",\\n  \\"CachePolicyId\\": \\"658327ea-f89d-4fab-a63d-7e88639e58f6\\"\\n}\'\\n  );\\n\\nSELECT\\n  iasql_commit ();\\n```\\n\\nWe can access our website through the CloudFront distribution domain name. To get it, we can simply run the following query:\\n\\n```sql title=\\"Get the Distribution Domain Name\\"\\nSELECT\\n  domain_name\\nFROM\\n  distribution\\nWHERE\\n  caller_reference = \'my-website\';\\n```\\n\\nIt supports HTTPS, it\'s fast, it\'s cheaper than directly serving on S3, and it can be easily connected to a Route53 domain."},{"id":"beta","metadata":{"permalink":"/blog/beta","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/updates/beta.md","source":"@site/blog/updates/beta.md","title":"IaSQL is in beta!","description":"IaSQL is in beta: AWS multiregion + infra changes as transactions + smooth local setup","date":"2023-02-16T00:00:00.000Z","formattedDate":"February 16, 2023","tags":[{"label":"updates","permalink":"/blog/tags/updates"}],"readingTime":4.11,"hasTruncateMarker":true,"authors":[{"name":"Yolanda Robla","imageURL":"https://github.com/yrobla.png","key":"yrobla"},{"name":"Mohammad Teimori Pabandi","imageURL":"https://github.com/mtp1376.png","key":"mtp1376"},{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"},{"name":"David Ellis","imageURL":"https://github.com/dfellis.png","key":"dfellis"},{"name":"Alejandro Guillen","imageURL":"https://github.com/aguillenv.png","key":"aguillenv"}],"frontMatter":{"slug":"beta","title":"IaSQL is in beta!","description":"IaSQL is in beta: AWS multiregion + infra changes as transactions + smooth local setup","image":"https://iasql.com/img/iasql-connector_dark.gif","date":"2023-02-16T00:00:00.000Z","authors":["yrobla","mtp1376","depombo","dfellis","aguillenv"],"tags":["updates"]},"prevItem":{"title":"Deploy a Static Website on AWS using SQL","permalink":"/blog/deploy-static-website"},"nextItem":{"title":"Save on AWS by deleting untagged ECR images","permalink":"/blog/ecr-save"}},"content":"IaSQL lets developers manage their cloud infrastructure as data in PostgreSQL as an alternative to the AWS console and infrastructure as code (IaC) tools like Pulumi and Terraform. We open-sourced IaSQL\'s Alpha version (v0.0.x) in [April 2022](/blog/os-iasql). We still have a long way to go, but we feel ready for what is next. Today, we\u2019re moving IaSQL to Beta (v0.x)!\\n\\n\x3c!--truncate--\x3e\\n\\nWe\u2019ve been fortunate to work with lots of early adopters who helped us shape the product and prioritize the features to build. We have been quietly fixing bugs and adding goodies to IaSQL non-stop for the past few months. Some of the bugs have led to some difficult outages. We love open source and that includes airing our dirty laundry. We keep our postmortems [here](https://github.com/alantech/iasql/tree/main/postmortems) in case you are curious.\\n\\n<img width={340} src={\'https://media.tenor.com/znsYWE0DQKsAAAAC/cats-laundry.gif\'} />\\n\\nAdditionally, we completely redid our architecture to scale and our UX to make it more intuitive and just plain simpler. More on that later. Several hundred SQL queries and cloud resources have been created on top of IaSQL. Blood, sweat, and tears went into the Alpha phase. Okay, that may be an overdramatization, but it did take a lot of thoughtful hours from us to help you manage cloud infrastructure more seamlessly.\\n\\n<img width={340} src={\'https://media.tenor.com/Nbv1SysxlrUAAAAC/heavenly-joy-jerkins-i-am-so-excited.gif\'} />\\n\\n## Alpha phase in numbers\u200b\\n\\n- 26 Alpha versions\\n- 170 databases created\\n- 199 GitHub stars\\n- 11 GitHub contributors\\n- 481 GitHub issues closed\\n- 140,755 lines of code and markdown\\n- 24 AWS services covered\\n- 90 Discord members\\n\\n## New features\\n\\nHere are the new features that ship in the Beta versions of IaSQL:\\n\\n### \ud83c\udfe1 Home is where your local env is\\n\\nWe made IaSQL easier to run locally by bundling up our dashboard into the IaSQL docker container and publishing it to [Dockerhub](https://hub.docker.com/r/iasql/iasql). This makes IaSQL easier to try out without having your cloud credentials ever leave your local environment. It is as simple as running the command below and going to `http://localhost:9876` on your preferred browser.\\n\\n```bash\\ndocker run --pull=always -p 9876:9876 -p 5432:5432 --name iasql iasql/iasql\\n```\\n\\n### \ud83c\udf9b\ufe0f AWS Multiregion\\n\\nSupport for multiple AWS regions with default region behavior in part because we also hate changing regions in the AWS console. The default region is defined when connecting your database to your AWS account. Thereon, IaSQL\'s data model will assume the default data model unless you explicitly override it in the column that represents your cloud resource.\\n\\n[Learn more about it in the RFC for this feature &#8594;](https://github.com/alantech/iasql/blob/main/rfcs/003%20-%20Multi-Region%20Support%20RFC.md)\\n\\n### \ud83e\ude84 Infrastructure changes as transactions, please\\n\\nWe redid the UX to allow handling infrastructure changes automatically and for delicate, or complex, changes to your cloud account use an [IaSQL transaction](/docs/transaction) akin to transactions in a regular database. Do you want to programmatically modify your infra or control plane? We got you!\\n\\n[Learn more about it in the RFC for this feature &#8594;](https://github.com/alantech/iasql/blob/main/rfcs/004%20-%20Continuous%20Two-Way%20Synchronization%20RFC.md)\\n\\n### \ud83c\udf9a\ufe0f Moar coverage of AWS services\\n\\nIncreased AWS service coverage for EC2, CodeDeploy, CodeBuild, CodePipeline, SNS, ACM, Route53 amongst a few others. Additionally, we have a new `aws_sdk` module that lets you invoke the AWS SDK directly using PostgreSQL functions with added type safety.\\n\\n[See an up-to-date list of covered AWS services &#8594;](/docs/modules/)\\n\\n### \ud83d\udca8 Breeze through a simplified AWS\\n\\nAWS is well... complicated. Our modules let you create, update, and delete your cloud resources as relational tables with the configurability AWS provides, but sometimes those details are not relevant to what you are trying to accomplish. So we have developed simplified modules that focus on specific use cases. For instance deploying a docker container to ECS and exposing it to the internet which is not just ECS but involves ECR, ECS, ACM, and Route53. These simplified modules are written in pure SQL on top of the existing IaSQL modules and are meant to abstract the complexity of coordinating multiple AWS services while still letting you peek under the hood when needed. Think of a simplified module as a PaaS hosted in your AWS account that is built on top of known AWS services but also lets you eject back into these AWS services if necessary.\\n\\n[Learn more about simplified modules here &#8594;](/blog/ecs-simplified)\\n\\n### \ud83d\udcc8 Scale for what?\\n\\nRe-architected the product to scale the SaaS beyond a handful of users and allow automatic database version upgrades for [modules](/docs/modules). \\n\\n[Learn more about it in the RFC for this feature &#8594;](https://github.com/iasql/iasql/blob/main/rfcs/005%20-%20Unsurprising%20Functions%20and%20Scalability%20RFC.md)\\n\\n## What\u2019s next?\u200b\\n\\nThe next features are going to be about making IaSQL easier to use:\\n- more [examples](/blog/tags/tutorial/)\\n- SQL templates for common security vulnerabilities and cost optimizations in AWS\\n- continuously improving our documentation\\n\\nLonger term, we\u2019ll add support for 3rd party high-level modules, extensive support for AWS, and support for more cloud providers. If there is something in particular you would like to see please drop us a line on [Discord](https://discord.iasql.com) or email via hello at our domain.\\n\\n*Want to stay in the loop? \u2192 [Join our newsletter!](/updates)*"},{"id":"ecr-save","metadata":{"permalink":"/blog/ecr-save","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/optimizations/save-on-ecr.mdx","source":"@site/blog/optimizations/save-on-ecr.mdx","title":"Save on AWS by deleting untagged ECR images","description":"IaSQL is an open-source software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. In this post, we are going to learn how untagged ECR images can rack up your AWS bill unnecessarily and how to get rid of unused repository images with a single query in IaSQL: DELETE FROM repository_images WHERE tag = \'\';  Every time you want to deploy a new version of your code to an ECS container or EKS pod, you build and push a new docker image from your code into an ECR container repository. The latest image in an ECR repository is the only one in use. Stale images remain in the repository and accumulate over long periods by normal usage of ECS, or EKS, as there is typically no workflow to delete untagged images. However, there is a gotcha. AWS charges $0.10 per GB per month for images stored in private or public ECR repositories per the pricing page. This doesn\'t sound like a lot, but for context, the size of the IaSQL docker image is around 2 GB. We have a CI job that deploys our staging environment every time we land a pull request to the main branch of our repository. This added up to 845 deploys over only a few months. Those 845 images, which have since been deleted, summed up to roughly 1700 GB which was $170 per month in our case. However, companies with lots of microservices on ECS or large docker containers can have many gigabytes of unused storage that can come out to hundreds or thousands of dollars per month of unnecessary AWS spend.","date":"2023-02-15T00:00:00.000Z","formattedDate":"February 15, 2023","tags":[{"label":"optimizations","permalink":"/blog/tags/optimizations"}],"readingTime":2.975,"hasTruncateMarker":true,"authors":[{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"}],"frontMatter":{"slug":"ecr-save","title":"Save on AWS by deleting untagged ECR images","authors":["depombo"],"date":"2023-02-15T00:00:00.000Z","tags":["optimizations"]},"prevItem":{"title":"IaSQL is in beta!","permalink":"/blog/beta"},"nextItem":{"title":"Deploying to ECS, Simplified!","permalink":"/blog/ecs-simplified"}},"content":"IaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. In this post, we are going to learn how untagged ECR images can rack up your AWS bill unnecessarily and how to get rid of unused repository images with a single query in IaSQL: `DELETE FROM repository_images WHERE tag = \'<untagged>\';` \x3c!--truncate--\x3e Every time you want to deploy a new version of your code to an ECS container or EKS pod, you build and push a new docker image from your code into an ECR container repository. The latest image in an ECR repository is the only one in use. Stale images remain in the repository and accumulate over long periods by normal usage of ECS, or EKS, as there is typically no workflow to delete untagged images. However, there is a gotcha. AWS charges $0.10 per GB per month for images stored in private or public ECR repositories per the [pricing page](https://aws.amazon.com/ecr/pricing/). This doesn\'t sound like a lot, but for context, the size of the IaSQL docker image is around 2 GB. We have a CI job that deploys our staging environment every time we land a pull request to the main branch of our repository. This added up to 845 deploys over only a few months. Those 845 images, which have since been deleted, summed up to roughly 1700 GB which was $170 per month in our case. However, companies with lots of microservices on ECS or large docker containers can have many gigabytes of unused storage that can come out to hundreds or thousands of dollars per month of unnecessary AWS spend.\\n\\nHow do you get rid of untagged ECR images though? Deleting hundreds, or thousands, of container images across ECR repositories is quite tedious through the AWS console, which involves multiple clicks per deleted image. There are a few other options. IaC tools require state file manipulation, or it is not possible at all as ECR images are not typically modeled in the infrastructure declaration as they are often generated via CI and are quite numerous. Cloud Query and Steampipe let you query your cloud and inspect your ECR images, but do not let you delete the images or any part of your cloud account for that matter as they are read-only. The most common solution is often a script that calls the AWS SDK or CLI directly. IaSQL lets you delete untagged images to eliminate unnecessary AWS costs using the following query:\\n\\n```sql title=\\"Delete untagged ECR images\\"\\n\\nSELECT * FROM iasql_install(\'aws_ecr\');\\n\\nDELETE FROM repository_images WHERE tag = \'<untagged>\';\\n```\\n\\nIt is also possible to delete old images, or images past a certain size This query deletes unused images pushed to the repository before 2023 that are bigger than 10 GB:\\n\\n```sql\\nDELETE FROM repository_images WHERE tag = \'<untagged>\' AND pushed_at < \'2023-01-01\' AND size_in_mb = 10000;\\n```\\n\\nThere is a way to delete all *unused* images if you are using ECR with ECS exclusively:\\n\\n```sql title=\\"Delete unused ECR images\\"\\n\\nSELECT * FROM iasql_install(\'aws_ecs_fargate\');\\n\\nDELETE FROM repository_images;\\n```\\n\\nAfter installing the `aws_ecs_fargate` [module](/docs/module), which installs the dependant `aws_ecr` module if it\'s not already installed, the schema relations between the AWS ECS service will not let you delete ECR images currently in use and will reinstate the ones that are currently active.\\n\\nQuestions or issues with AWS? Join our [Discord channel](https://discord.iasql.com) to ask away, or just to let us know if you would like to see more AWS cost optimizations like this one."},{"id":"ecs-simplified","metadata":{"permalink":"/blog/ecs-simplified","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/ecs-simplified.mdx","source":"@site/blog/tutorials/ecs-simplified.mdx","title":"Deploying to ECS, Simplified!","description":"IaSQL is an open-source software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. In this post, we\'re going to discover an IaSQL module that\'s built to make deploying to ECS, simplified. Most of the details for deploying a container to AWS ECS are the same (load balancers, security groups, IAM roles, etc), and we have created the awsecssimplified module for you so that you can give it any Github repo with a Dockerfile and get your app deployed to ECS in the fastest time possible, with scalability available! All the needed resources are going to be created automatically in your AWS account, and you\'ll have full access to the details while you\'re gaining the benefit of a higher-level simple deployment.","date":"2023-02-13T00:00:00.000Z","formattedDate":"February 13, 2023","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":7.76,"hasTruncateMarker":true,"authors":[{"name":"Mohammad Teimori Pabandi","imageURL":"https://github.com/mtp1376.png","key":"mtp1376"}],"frontMatter":{"slug":"ecs-simplified","title":"Deploying to ECS, Simplified!","date":"2023-02-13T00:00:00.000Z","authors":["mtp1376"],"tags":["tutorial"]},"prevItem":{"title":"Save on AWS by deleting untagged ECR images","permalink":"/blog/ecr-save"},"nextItem":{"title":"Deploy containerized app to Fargate (Django)","permalink":"/blog/django"}},"content":"IaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. In this post, we\'re going to discover an [IaSQL module](docs/module) that\'s built to make deploying to ECS, simplified. Most of the details for deploying a container to AWS ECS are the same (load balancers, security groups, IAM roles, etc), and we have created the `aws_ecs_simplified` module for you so that you can give it any Github repo with a `Dockerfile` and get your app deployed to ECS in the fastest time possible, with scalability available! All the needed resources are going to be created automatically in your AWS account, and you\'ll have full access to the details while you\'re gaining the benefit of a higher-level simple deployment.\\n\\n\x3c!--truncate--\x3e\\n\\nIf you have ever tried to deploy your containerized application to ECS, you know that it\'s not going to be an easy click-to-deploy journey. To get your application up and running on ECS, you have to go through a bunch of resource creation. You\'ll need to:\\n\\n- Deploy a load balancer as the point-of-contact for your app\\n- Create a target group for the load balancer and register the ECS tasks in it\\n- Add a new listener to your load balancer and connect it to the target group\\n- Create a security group, and you need to allow the port your app is listening on in that security group\\n- Attach the security group above to your load balancer\\n- Create a CloudWatch log group for your ECS task\\n- Create an ECS cluster, and definitely the task definition as well\\n- Oh, and create an ECR repository to push your images to be run on the container\\n\\nI\'m not going to continue this long list, since I\'ve already got a headache. Doing those steps manually is going to give you a headache as well, so why bother doing all those steps yourself and risking different errors you might face when deploying your containerized app? You don\'t really have the time for the random IAM-related errors AWS is demanding you to resolve. Besides, you already have your codebase ready and the `Dockerfile` is there, so why not just run a simple command doing something that should be simply done?\\n\\n## ** An Example Usage of the `aws_ecs_simplified` Module **\\n\\nLet\'s say we are going to deploy [this simple Express.js app](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate/prisma/app) to the ECS. It has a `Dockerfile` and `package.json` that installs `express` on `npm install`. `npm start` then starts the Express server which listens on port `8088`.\\n\\n:::note\\n`aws_ecs_simplified` is a high-level module we have created to make scalable ECS deployments easier. For more info on high-level vs low-level modules, you can check [this guide](https://iasql.com/docs/low-level-vs-high-level/).\\n:::\\n\\nLet\'s go and deploy the above app to your AWS account. Don\'t worry if you don\'t have an IaSQL database already, you can run IaSQL locally and run one locally for free [here](/docs/).\\n\\n```sql title=\\"Deploy a simple Express.js app from a Github repository to ECS\\" showLineNumbers\\nSELECT\\n  iasql_install (\'aws_ecs_simplified\', \'aws_codebuild\');\\n\\nSELECT\\n  iasql_begin ();\\n\\nINSERT INTO\\n  ecs_simplified (app_name, app_port, image_tag, public_ip)\\nVALUES\\n  (\'simple-express\', 8088, \'latest\', TRUE);\\n\\nSELECT\\n  iasql_commit ();\\n\\nSELECT\\n  ecr_build (\\n    \'https://github.com/alantech/iasql/\', -- the Github repo URL\\n    (\\n      SELECT\\n        id\\n      FROM\\n        repository\\n      WHERE\\n        repository_name = \'simple-express-repository\'\\n    )::VARCHAR(255), -- ECR repo for the image to be pushed\\n    \'./examples/ecs-fargate/prisma/app\', -- the subdirectory in Github repo\\n    \'main\', -- the Github branch or ref\\n    NULL -- Github personal access token - can be omitted if public repository\\n  );\\n```\\n\\nThat\'s it! Now wait for some time and your app is deployed! While your app is being deployed, let\'s go through the commands we executed in more depth:\\n\\n```sql\\nSELECT\\n  iasql_install (\'aws_ecs_simplified\', \'aws_codebuild\');\\n```\\n\\n- This command installs the `aws_ecs_simplified` high-level module. We\u2013 at IaSQL\u2013 have created that module to make it easy to deploy containerized apps to ECS. The code for it is [here](https://github.com/alantech/iasql/blob/v0.0.22/src/modules/0.0.23/aws_ecs_simplified/sql/after_install.sql). But IaSQL is so flexible that anyone can create their own high-level (and of course, low-level) modules and add it to IaSQL.\\n\\n```sql\\nINSERT INTO\\n  ecs_simplified (app_name, app_port, image_tag, public_ip)\\nVALUES\\n  (\'simple-express\', 8088, \'latest\', TRUE);\\n```\\n\\n- This command creates a new `ecs_simplified` app by inserting a new row into the `ecs_simplified` table. Seems pretty easy, right? But under the hood, it\'s creating all the necessary resources like load balancers, security groups, IAM roles, etc.\\n- You can manually check the tables to see what resources are being created. For example, looking at the `load_balancer` table you\'ll see a load balancer named `simple-express-load-balancer` is inserted automatically by running the above insert command.\\n- The `iasql_begin()` and `iasql_commit()` functions are IaSQL RPCs that are used to start and then end a transaction. We use those two functions to bundle changes to be pushed to the cloud immediately. If you don\'t wrap the changes in a transaction, they\'ll be applied to the cloud in an eventually-consistent way.\\n- For more info on the `iasql_begin()` and `iasql_commit()` commands, check [this guide](https://iasql.com/docs/transaction/) on how it works.\\n- After the transaction is finished all the necessary resources are now created on the cloud, and their cloud-defined values are synced back to the database, so you can see the ARNs in the database. You can verify this by looking at different tables, eg. `iam_role`.\\n- To get your load balancer address, you can easily run `SELECT load_balancer_dns FROM ecs_simplified WHERE app_name = \'simple-express\'` query and get the URL to access your app.\\n- Now ECS is waiting for an image to be pushed to your ECR repository to run it. You can get the URI for the ECR repository by running the `SELECT repository_uri FROM ecs_simplified WHERE app_name = \'simple-express\'` query. You _could_ build your docker image locally and then follow [Steps 2 and 4 from this guide](https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html) to connect your local docker CLI to your ECR repository and push that docker image into your ECR repository, but we have a simpler solution next.\\n- In the next step, we\'ll automatically build an image for the code in Github repo and then push it to this URI (all through SQL and using another high-level function named `ecr_build`).\\n\\n```sql\\nSELECT\\n  ecr_build (\\n    \'https://github.com/alantech/iasql/\', -- the Github repo URL\\n    (\\n      SELECT\\n        id\\n      FROM\\n        repository\\n      WHERE\\n        repository_name = \'simple-express-repository\'\\n    )::VARCHAR(255), -- ECR repo for the image to be pushed\\n    \'./examples/ecs-fargate/prisma/app\', -- the subdirectory in Github repo\\n    \'main\', -- the Github branch or ref\\n    NULL -- Github personal access token - can be omitted if public repository\\n  );\\n```\\n\\n- This command tells IaSQL to clone the `iasql` repository, build an image on the subdirectory specified, and then push it to the ECR repository created earlier by the `aws_ecs_simplified` module. Running the above command will automatically create a CodeBuild project and the related roles, etc. Then it\'ll start a build, and after it\'s successful all the created resources are deleted to ensure there won\'t be any additional charges to your AWS account.\\n- To access your app on the cloud, get the load balancer address and use your browser to access the live version of it:\\n\\n```sql\\nSELECT\\n  load_balancer_dns\\nFROM\\n  ecs_simplified\\nWHERE\\n  app_name = \'simple-express\';\\n```\\n\\nThen you can check if the server is running on the `<load_balancer_dns value>:8088/health` address.\\n\\n## Low-level Access to Resources\\n\\nSo the `aws_ecs_simplified` module simplifies things, right? But what if you still need the level of control you had when you were doing all the steps manually? The traditional PaaS trade-off is that you can\'t grow your app beyond the built-in limitations as you don\'t have access to all the small details. The IaSQL approach is not limited in that way.\\n\\nLet\'s say you want your ECS container to be able to use the AWS CLI to provision an EC2 instance, and for that purpose its IAM role needs `AmazonEC2FullAccess` policy to work properly. `aws_ecs_simplified` does not have a column to configure such a thing, but that doesn\'t mean we\'re stuck.\\n\\nThe good news is that you still have the full control over all resources in the deepest details. Let\'s fix your app\'s IAM role access by attaching the needed policy to its IAM role:\\n\\n```sql\\nUPDATE\\n  iam_role\\nSET\\n  attached_policies_arns = attached_policies_arns || \'arn:aws:iam::aws:policy/AmazonEC2FullAccess\' -- attached_policies_arns is of text[] type\\nWHERE\\n  role_name = \'simple-express-ecs-task-exec-role\';\\n```\\n\\nYou want additional rules for the container\'s security group? No problem! Just write the SQL and execute it, and it will be applied to the cloud within seconds. You want 3 copies of your container to be kept running with a round-robin load balancing on them? It\'s already there, just do an `UPDATE ecs_simplified SET desired_count = 3 WHERE app_name = \'simple_express\';` and it\'s there for you.\\n\\nWith IaSQL and its flexibility, you can benefit from both the high-level and low-level operations. We have created the `aws_ecs_simplified` module to show the flexibility and power of IaSQL, but the possibilities are endless. IaSQL is also an open-source project, meaning that you can use it to build your very own modules on top of it. If you\'re into the idea of empowering other developers to do complex infrastructure tasks simply, why don\'t you take a look at our [contributing guide](https://github.com/alantech/iasql/blob/main/CONTRIBUTING.md) and join our [Discord channel](https://discord.iasql.com)? We\'ll thoroughly answer any of your questions regarding the usage or development of IaSQL. Looking forward to seeing you in our small, but great community."},{"id":"/django","metadata":{"permalink":"/blog/django","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/django.mdx","source":"@site/blog/tutorials/django.mdx","title":"Deploy containerized app to Fargate (Django)","description":"In this tutorial, we will run Django SQL migrations on top of IaSQL to deploy an HTTP server within a docker container on your AWS account using ECS, ECR, and ELB. The container image will be hosted as a public repository in ECR and deployed to ECS using Fargate.","date":"2022-12-16T00:00:00.000Z","formattedDate":"December 16, 2022","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":6.965,"hasTruncateMarker":true,"authors":[],"frontMatter":{"sidebar_position":3,"title":"Deploy containerized app to Fargate (Django)","slug":"/django","date":"2022-12-16T00:00:00.000Z","tags":["tutorial"]},"prevItem":{"title":"Deploying to ECS, Simplified!","permalink":"/blog/ecs-simplified"},"nextItem":{"title":"Deploy containerized app to Fargate (Prisma)","permalink":"/blog/prisma"}},"content":"In this tutorial, we will run [Django SQL migrations](https://docs.djangoproject.com/en/4.0/topics/migrations/) on top of IaSQL to deploy an HTTP server within a docker container on your AWS account using ECS, ECR, and ELB. The container image will be hosted as a public repository in ECR and deployed to ECS using Fargate.\\n\\n\\n\x3c!--truncate--\x3e\\n\\nIaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. The code for this tutorial lives in this part of the [repository](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate/django/app/infra/migrations/0003_initial.py).\\n\\n## Start managing an AWS account with a PostgreSQL IaSQL db\\n\\nFirst, make sure you have an [IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html) in AWS or create one with **Programmatic access** through the [console/UI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) or [CLI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_cliwpsapi). Ensure that the IAM role has sufficient permissions to deploy and manage all your infrastructure resources.\\n\\nThere are two parts to each [access key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys), which you\u2019ll see in the IAM console/CLI after you create it, an id and a secret. Input these in the connect account modal:\\n\\nimport useBaseUrl from \'@docusaurus/useBaseUrl\';\\nimport ThemedImage from \'@theme/ThemedImage\';\\n\\n<ThemedImage\\n  alt=\\"Connect\\"\\n  style={{width: \'440\'}}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/connect.png\'),\\n    dark: useBaseUrl(\'/screenshots/connect_dark.png\'),\\n  }}\\n/>\\n\\n\\nIf you use the [AWS CLI](https://docs.aws.amazon.com/cli/), you can look at the [credentials configured locally](https://docs.aws.amazon.com/sdkref/latest/guide/file-location.html). In macOS and Linux this is as simple as:\\n\\n```bash\\n$ cat ~/.aws/credentials\\n[default]\\naws_access_key_id = <YOUR_ACCESS_KEY_ID>\\naws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>\\n```\\n\\nYou will be able to see your PostgreSQL connection information when you press Connect.\\n\\n\\n<ThemedImage\\n  alt=\\"Credentials\\"\\n  style={{width: \'440\', border: \'2px solid rgba(52, 52, 52, 0.1)\' }}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/credentials.png\'),\\n    dark: useBaseUrl(\'/screenshots/credentials_dark.png\'),\\n  }}\\n/>\\n\\nMake sure to copy the PostgreSQL connection string as you will not see it again.\\n\\n## Add the necessary cloud services to the PostgreSQL database\\n\\n\x3c!-- TODO after release [connect](/docs/postgres) --\x3e\\n1. Many different clients can be used to connect to a PostgreSQL database. For this tutorial, we\'ll use the standard `psql` CLI client. If you need to install `psql`, follow the instructions for your corresponding OS [here](https://www.postgresql.org/download/).\\n\\n2. The first migration calls the `iasql_install` SQL function to install the ECS simplified [module](/docs/module) into the PostgreSQL database.\\n\\n```sql title=\\"psql postgres://d0va6ywg:nfdDh#EP4CyzveFr@localhost:5432/_4b2bb09a59a411e4 -c\\"\\nSELECT\\n  *\\nFROM\\n  iasql_install (\'aws_ecs_simplified\', \'aws_codebuild\');\\n```\\n\\nIf the function call is successful, it will return a virtual table with a record for each new table in your database under `created_table_name` and the number of existing resources or records imported from the account under `record_count`.\\n\\n```sql\\n       module_name        |      created_table_name       | record_count\\n--------------------------+-------------------------------+--------------\\n aws_cloudwatch           | log_group                     |            0\\n aws_ecr                  | public_repository             |            0\\n aws_ecr                  | repository                    |            1\\n aws_ecr                  | repository_policy             |            0\\n aws_security_group       | security_group                |            2\\n aws_security_group       | security_group_rule           |            0\\n aws_vpc                  | vpc                           |            1\\n aws_vpc                  | subnet                        |            3\\n aws_elb                  | load_balancer                 |            0\\n aws_elb                  | target_group                  |            0\\n aws_elb                  | listener                      |            0\\n aws_elb                  | load_balancer_security_groups |            0\\n aws_ecs_fargate          | cluster                       |            0\\n aws_ecs_fargate          | service                       |            0\\n aws_ecs_fargate          | task_definition               |            0\\n aws_ecs_fargate          | container_definition          |            0\\n aws_ecs_fargate          | service_security_groups       |            0\\n```\\n\\n## Connect to the PostgreSQL db and provision cloud resources in your AWS account\\n\\n1. Get a local copy of the [ECS Fargate examples](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate)\\n\\n2. (Optional) Create and activate a virtual environment to install python dependencies\\n\\n   ```bash\\n   python -m venv <env-name>\\n   source <env-name>/bin/activate\\n   ```\\n\\n3. Install the project dependencies under the `django/app` folder\\n\\n   ```bash\\n   pip install -r requirements.txt\\n   ```\\n\\n4. Create a `.env` file with the connection parameters provided on db creation. In this case:\\n\\n   ```title=\\"django/app/.env\\"\\n   AWS_REGION=eu-west-2\\n   DB_NAME=_3ba201e349a11daf\\n   DB_USER=qpp3pzqb\\n   DB_PASSWORD=LN6jnHfhRJTBD6ia\\n   ```\\n\\n5. (Optional) Set the desired project name that your resources will be named after by changing the `IASQL_PROJECT_NAME` in the `my_project/app/app/settings.py`. If the name is not changed, `quickstart` will be used.\\n\\n   :::note\\n\\n   The `project-name` can only contain alphanumeric characters and hyphens(-) because it will be used to name the load balancer\\n\\n   :::\\n\\n6. Per the [Django database documentation](https://docs.djangoproject.com/en/4.0/ref/databases/#postgresql-connection-settings-1), to connect to a new database you have to update the `DATABASES` in the `my_project/app/app/settings.py` file. This is already configured in the example project.\\n\\n   ```python title=\\"django/app/app/settings.py\\"\\n   DATABASES = {\\n       ...\\n       \'infra\': {\\n           \'ENGINE\': \'django.db.backends.postgresql\',\\n           \'NAME\': env(\'DB_NAME\'),\\n           \'USER\': env(\'DB_USER\'),\\n           \'PASSWORD\': env(\'DB_PASSWORD\'),\\n           \'HOST\': \'localhost\',\\n           \'PORT\': \'5432\',\\n       }\\n   }\\n   ```\\n\\n### If you are using the template example go to step 9. The following steps explains how to instrospect an existing DB in Django.\\n\\n7. The second migration corresponds to the Django models introspected from the modules that have been installed in the database. To introspect the schema from your database run the following command. More information [here](https://docs.djangoproject.com/en/4.0/howto/legacy-databases/).\\n\\n```bash\\npython manage.py inspectdb --database=infra > infra/models.py\\n```\\n\\n:::note\\n\\nAfter running the `inspectdb` command you will need to tweak the models Django generated until they work the way you\u2019d like.\\nIn our case you will have to modify the `my_project/app/infra/models.py` file as follow:\\n\\n1. Replace `CharField` with `TextField`\\n2. Remove all `max_length=-1`. Helpful regex for a replacement: `[\\\\s,]*max_length=-1[,\\\\s]*`\\n3. Add the following import `from django.contrib.postgres.fields import ArrayField`\\n4. Replace in the `Service` class the `subnets` property with `subnets = ArrayField(models.TextField())`\\n5. Replace in the `Role` class the `attached_policies_arns` property with `attached_policies_arns = ArrayField(models.TextField())`\\n6. Add `related_name` argument to the definition for `IasqlDependencies.dependency`. (`dependency = models.ForeignKey(\'IasqlModule\', models.DO_NOTHING, db_column=\'dependency\', related_name=\'module\')`)\\n7. Add `related_name` argument to the definition for `TaskDefinition.execution_role_name`. (`execution_role_name = models.ForeignKey(Role, models.DO_NOTHING, db_column=\'execution_role_name\', blank=True, null=True, related_name=\'execution_role_name\')`)\\n8. Add `related_name` argument to the definition for `TaskDefinition.task_role_name`. (`task_role_name = models.ForeignKey(Role, models.DO_NOTHING, db_column=\'task_role_name\', blank=True, null=True, related_name=\'task_role_name\')`)\\n\\n:::\\n\\n9. After instrospecting the db you will need to generate the migration so you can have the `my_project/app/infra/migrations/0002_inspectdb.py` file.\\n\\n   ```bash\\n   python manage.py makemigrations --name inspectdb infra\\n   ```\\n\\n   :::caution\\n\\n   If you install or uninstall IaSQL [modules](docs/module) the database schema will change and you will need to run steps 7 and 8 to\\n   introspect the correct schema once again.\\n\\n   :::\\n\\n10. Now you can use IaSQL models to create your resources. Run the existing migrations with:\\n\\n    ```bash\\n    python manage.py migrate --database infra infra\\n    ```\\n\\n    The operations of the `my_project/app/infra/migrations/0003_initial.py` migration will apply the changes described in the PostgreSQL db to your cloud account which will take a few minutes waiting for AWS\\n\\n    ```python title=\\"my_project/app/infra/migrations/0003_initial.py\\"\\n    ...\\n    operations = [\\n        migrations.RunPython(code=quickstart_up, reverse_code=apply),\\n        migrations.RunPython(code=apply, reverse_code=quickstart_down),\\n    ]\\n    ```\\n\\nIf the function call is successful, it will return a list of dicts with each cloud resource that has been created, deleted or updated.\\n\\n```python\\n[{\'action\': \'create\', \'table_name\': \'log_group\', \'id\': None, \'description\': \'quickstart-log-group\'}, {\'action\': \'create\', \'table_name\': \'repository\', \'id\': None, \'description\': \'quickstart-repository\'}, {\'action\': \'create\', \'table_name\': \'iam_role\', \'id\': None, \'description\': \'quickstart-ecs-task-exec-role\'}, {\'action\': \'create\', \'table_name\': \'security_group\', \'id\': 31, \'description\': \'31\'}, {\'action\': \'create\', \'table_name\': \'security_group_rule\', \'id\': 48, \'description\': \'48\'}, {\'action\': \'create\', \'table_name\': \'security_group_rule\', \'id\': 49, \'description\': \'49\'}, {\'action\': \'create\', \'table_name\': \'listener\', \'id\': 16, \'description\': \'16\'}, {\'action\': \'create\', \'table_name\': \'load_balancer\', \'id\': None, \'description\': \'quickstart-load-balancer\'}, {\'action\': \'create\', \'table_name\': \'target_group\', \'id\': None, \'description\': \'quickstart-target\'}, {\'action\': \'create\', \'table_name\': \'cluster\', \'id\': None, \'description\': \'quickstart-cluster\'}, {\'action\': \'create\', \'table_name\': \'task_definition\', \'id\': 16, \'description\': \'16\'}, {\'action\': \'create\', \'table_name\': \'service\', \'id\': None, \'description\': \'quickstart-service\'}, {\'action\': \'delete\', \'table_name\': \'security_group_rule\', \'id\': None, \'description\': \'sgr-024274a604968919e\'}]\\n```\\n\\n## Login, build and push your code to the container registry\\n\\nPreviously, you needed to manually build and push your image to the ECR. But recently we\'ve added the high-level `ecr_build` SQL function which does all those steps automatically. It will do the following:\\n\\n- Pull the code from your Github repository\\n- Build the Docker image in the directory you\'ve specified\\n- Push the image to the ECR repository you\'ve provided\\n\\nAll of these steps will be done in a CodeBuild project in your AWS account. To use the `ecr_build` function, you can run:\\n\\n```sql\\nSELECT\\n  ecr_build (\\n    \'https://github.com/alantech/iasql/\', -- replace with your own Github repo if you want to use your own codebase\\n    (\\n      SELECT\\n        id\\n      FROM\\n        repository\\n      WHERE\\n        repository_name = \'quickstart-repository\'\\n    )::VARCHAR(255), -- replace quickstart if you\'ve changed the project name\\n    \'./examples/ecs-fargate/django/app\', -- the sub directory in the Github repo that the image should be built in\\n    \'main\', -- the Github repo branch name\\n    \'\' -- replace your github personal access token here if the repo is private\\n  );\\n```\\n\\nAfter running the above SQL command to completion, you can check the running app using the load balancer DNS name. To grab the name, run:\\n\\n```bash\\nQUICKSTART_LB_DNS=$(psql -At \'postgres://d0va6ywg:nfdDh#EP4CyzveFr@localhost:5432/_4b2bb09a59a411e4\' -c \\"\\nSELECT dns_name\\nFROM load_balancer\\nWHERE load_balancer_name = \'<project-name>-load-balancer\';\\")\\n```\\n\\nAnd then connect to your service!\\n\\n```\\ncurl ${QUICKSTART_LB_DNS}:8088/health\\n```\\n\\n## Delete Managed Cloud Resources\\n\\nDelete the resources created by this tutorial using the following SQL code:\\n\\n```sql title=\\"psql postgres://qpp3pzqb:LN6jnHfhRJTBD6ia@localhost:5432/_3ba201e349a11daf -c\\"\\nDELETE FROM\\n  repository_image\\nWHERE\\n  private_repository_id = (\\n    SELECT\\n      id\\n    FROM\\n      repository\\n    WHERE\\n      repository_name = \'quickstart-repository\'\\n  );\\n\\nDELETE FROM\\n  ecs_simplified\\nWHERE\\n  app_name = \'quickstart\';\\n```"},{"id":"/prisma","metadata":{"permalink":"/blog/prisma","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/prisma.mdx","source":"@site/blog/tutorials/prisma.mdx","title":"Deploy containerized app to Fargate (Prisma)","description":"In this tutorial, we will use a script that uses Prisma to introspect the schema of an IaSQL database and deploy a Node.js HTTP server within a docker container on your AWS account using Fargate ECS, IAM, ECR, and ELB. IaSQL is an open-source software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. The container image will be hosted as a private repository in ECR and deployed to ECS using Fargate.","date":"2022-12-15T00:00:00.000Z","formattedDate":"December 15, 2022","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[],"frontMatter":{"sidebar_position":2,"date":"2022-12-15T00:00:00.000Z","title":"Deploy containerized app to Fargate (Prisma)","slug":"/prisma","tags":["tutorial"]},"prevItem":{"title":"Deploy containerized app to Fargate (Django)","permalink":"/blog/django"},"nextItem":{"title":"Deploy containerized app to ECS Fargate","permalink":"/blog/fargate"}},"content":"In this tutorial, we will use a script that uses [Prisma](https://www.prisma.io) to introspect the schema of an IaSQL database and deploy a Node.js HTTP server within a docker container on your AWS account using Fargate ECS, IAM, ECR, and ELB. IaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database. The container image will be hosted as a private repository in ECR and deployed to ECS using Fargate.\\n\\nThe code for this tutorial lives in this part of the [repository](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate/prisma/infra/index.js).\\n\\n\x3c!--truncate--\x3e\\n\\n## Start managing an AWS account with a PostgreSQL IaSQL db\\n\\nFirst, make sure you have an [IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html) in AWS or create one with **Programmatic access** through the [console/UI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) or [CLI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_cliwpsapi). Ensure that the IAM role has sufficient permissions to deploy and manage all your infrastructure resources.\\n\\nThere are two parts to each [access key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys), which you\u2019ll see in the IAM console/CLI after you create it, an id and a secret. Input these in the connect account modal:\\n\\nimport useBaseUrl from \'@docusaurus/useBaseUrl\';\\nimport ThemedImage from \'@theme/ThemedImage\';\\n\\n<ThemedImage\\n  alt=\\"Connect\\"\\n  style={{width: \'440\'}}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/connect.png\'),\\n    dark: useBaseUrl(\'/screenshots/connect_dark.png\'),\\n  }}\\n/>\\n\\nIf you use the [AWS CLI](https://docs.aws.amazon.com/cli/), you can look at the [credentials configured locally](https://docs.aws.amazon.com/sdkref/latest/guide/file-location.html). In macOS and Linux this is as simple as:\\n\\n```bash\\n$ cat ~/.aws/credentials\\n[default]\\naws_access_key_id = <YOUR_ACCESS_KEY_ID>\\naws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>\\n```\\nYou will be able to see your PostgreSQL connection information when you press Connect.\\n\\n<ThemedImage\\n  alt=\\"Credentials\\"\\n  style={{width: \'440\', border: \'2px solid rgba(52, 52, 52, 0.1)\' }}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/credentials.png\'),\\n    dark: useBaseUrl(\'/screenshots/credentials_dark.png\'),\\n  }}\\n/>\\n\\nMake sure to copy the PostgreSQL connection string as you will not see it again.\\n\\n## Add the necessary cloud services to the PostgreSQL database\\n\\nUse the `iasql_install` SQL function to install [modules](/docs/module) into the PostgreSQL database.\\n\\n```sql\\nSELECT * from iasql_install(\\n  \'aws_ecs_simplified\'\\n);\\n```\\n\\nIf the function call is successful, it will return a virtual table with a record for each new table in your database under `created_table_name` and the number of existing resources or records imported from the account under `record_count`.\\n\\n```sql\\n       module_name        |      created_table_name       | record_count\\n--------------------------+-------------------------------+--------------\\n aws_cloudwatch           | log_group                     |            0\\n aws_iam                  | iam_role                      |            0\\n aws_ecr                  | public_repository             |            0\\n aws_ecr                  | repository                    |            1\\n aws_ecr                  | repository_policy             |            0\\n aws_security_group       | security_group                |            2\\n aws_security_group       | security_group_rule           |            0\\n aws_vpc                  | vpc                           |            1\\n aws_vpc                  | subnet                        |            3\\n aws_elb                  | load_balancer                 |            0\\n aws_elb                  | target_group                  |            0\\n aws_elb                  | listener                      |            0\\n aws_elb                  | load_balancer_security_groups |            0\\n aws_ecs_fargate          | cluster                       |            0\\n aws_ecs_fargate          | service                       |            0\\n aws_ecs_fargate          | task_definition               |            0\\n aws_ecs_fargate          | container_definition          |            0\\n aws_ecs_fargate          | service_security_groups       |            0\\n aws_ecs_simplified       | ecs_simplified                |            0\\n(17 rows)\\n```\\n\\n## Connect to the PostgreSQL db and provision cloud resources in your AWS account\\n\\n1. Get a local copy of the [ECS Fargate examples code](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate/prisma)\\n\\n2. Install the Node.js project dependencies under the `prisma/infra` folder\\n\\n```bash\\ncd infra\\nnpm i\\n```\\n\\n3. Modify the [`.env file`](https://www.prisma.io/docs/guides/development-environment/environment-variables) that Prisma expects with the connection parameters provided on db creation. You\'ll need to add your [Github personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) for the `ecr_build` SQL function to be able to do the pull. Also, if you\'re going to deploy a codebase other than the default one, set the `REPO_URI` variable. In this case:\\n\\n```bash title=\\"prisma/infra/.env\\"\\nDATABASE_URL=\\"postgres://d0va6ywg:nfdDh#EP4CyzveFr@localhost:5432/_4b2bb09a59a411e4\\"\\nGH_PAT=ghp_XXX\\nREPO_URI=\\"https://github.com/alantech/iasql\\"\\n```\\n\\n4. (Optional) Set the desired project name that your resources will be named after by changing the `name` in the `my_project/infra/package.json`. If the name is not changed, `quickstart` will be used.\\n\\n:::note\\n\\nThe `project-name` can only contain alphanumeric characters and hyphens(-) because it will be used to name the load balancer\\n\\n:::\\n\\n5. Per the [Prisma quickstart to add an existing project](https://www.prisma.io/docs/getting-started/setup-prisma/add-to-existing-project/relational-databases/connect-your-database-node-postgres), create a basic `schema.prisma` file.\\n\\n```json title=\\"prisma/infra/prisma/schema.prisma\\"\\ndatasource db {\\n  provider = \\"postgresql\\"\\n  url      = env(\\"DATABASE_URL\\")\\n}\\n\\ngenerator client {\\n  provider = \\"prisma-client-js\\"\\n}\\n```\\n\\n6. Pull, or introspect, the schema from your database which will auto-populate the rest of the `schema.prisma` file\\n\\n```\\nnpx prisma db pull\\n```\\n\\n7. Now install and generate the Prisma client by the introspected `schema.prisma`\\n\\n```\\nnpx prisma generate\\n```\\n\\n:::caution\\n\\nIf you install or uninstall IaSQL [modules](/docs/module) the database schema will change and you will need to run steps 5 through 7 to\\nintrospect the correct schema once again.\\n\\n:::\\n\\n8. Run the existing script using the Prisma entities\\n\\n```bash\\nnode index.js\\n```\\n\\nThis will run the following [code](https://github.com/alantech/iasql/tree/main/examples/ecs-fargate/prisma/infra/index.js)\\n\\n```js title=\\"prisma/infra/index.js\\"\\nasync function main() {\\n  await prisma.$queryRaw`SELECT * FROM iasql_begin();`;\\n  const data = {\\n    app_name: APP_NAME,\\n    public_ip: true,\\n    app_port: PORT,\\n    image_tag: \'latest\',\\n  };\\n  await prisma.ecs_simplified.upsert({\\n    where: { app_name: APP_NAME },\\n    create: data,\\n    update: data,\\n  });\\n\\n  const commit = await prisma.$queryRaw`SELECT *\\n                                       from iasql_commit();`;\\n  console.dir(commit);\\n\\n  console.log(\'Using ecr_build to build the docker image and push it to ECR...\');\\n  const repoId = (await prisma.repository.findFirst({\\n    where: { repository_name: `${APP_NAME}-repository` },\\n    select: { id: true },\\n  })).id.toString();\\n  let repoUri;\\n  if (REPO_URI) // manual\\n    repoUri = REPO_URI;\\n  else if (GITHUB_SERVER_URL && GITHUB_REPOSITORY) // CI\\n    repoUri = `${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}`;\\n  else\\n    repoUri = \'https://github.com/alantech/iasql\'\\n  const image = await prisma.$queryRaw`SELECT ecr_build(\\n              ${repoUri},\\n              ${repoId},\\n              \'./examples/ecs-fargate/prisma/app\',\\n              ${GITHUB_REF},\\n              ${GH_PAT}\\n  );`;\\n  console.log(image);\\n}\\n```\\n\\nIt\'ll use the `ecs_simplified` module to create all the necessary AWS resources needed for you app to run (load balancer, ECR repository, IAM role, etc). If the function call is successful, it will return a virtual table with a record for each cloud resource that has been created, deleted or updated.\\n\\n```sql\\n action |    table_name       |   id   |      description      \\n--------+---------------------+--------+-----------------------\\n create | public_repository   |      2 | quickstart-repository\\n create | cluster             |      2 | 2\\n create | task_definition     |      2 | 2\\n create | service             |      2 | 2\\n create | listener            |      2 | 2\\n create | load_balancer       |      2 | 2\\n create | target_group        |      2 | 2\\n create | security_group      |      5 | 5\\n create | security_group_rule |      3 | 3\\n create | security_group_rule |      4 | 4\\n create | role                |        | ecsTaskExecRole\\n```\\n\\n## Login, build and push your code to the container registry\\n\\nPreviously, you needed to manually build and push your image to the ECR. But recently we\'ve added the high-level `ecr_build` SQL function which does all those steps automatically. It will do the following:\\n- Pull the code from your Github repository\\n- Build the Docker image in the directory you\'ve specified\\n- Push the image to the ECR repository you\'ve provided\\n\\nAll of these steps will be done in a CodeBuild project in your AWS account. To use the `ecr_build` function, you can run:\\n```sql\\nSELECT ecr_build(\\n   \'https://github.com/alantech/iasql/\', -- replace with your own Github repo if you want to use your own codebase\\n   (SELECT id\\n    FROM repository\\n    WHERE repository_name = \'quickstart-repository\')::varchar(255), -- replace quickstart if you\'ve changed the project name\\n   \'./examples/ecs-fargate/prisma/app\', -- the sub directory in the Github repo that the image should be built in\\n   \'main\', -- the Github repo branch name\\n   \'\' -- replace your github personal access token here if the repo is private\\n);\\n```\\nThat command is already being run in the `infra/index.js` script. So no need for extra steps if you\'re using it.\\n\\nAfter running the above SQL command to completion, you can check the running app using the load balancer DNS name. To grab the name, run:\\n```bash\\nQUICKSTART_LB_DNS=$(psql -At \'postgres://d0va6ywg:nfdDh#EP4CyzveFr@localhost:5432/_4b2bb09a59a411e4\' -c \\"\\nSELECT dns_name\\nFROM load_balancer\\nWHERE load_balancer_name = \'<project-name>-load-balancer\';\\")\\n```\\nAnd then connect to your service!\\n\\n```\\ncurl ${QUICKSTART_LB_DNS}:8088/health\\n```\\n\\n## Delete Managed Cloud Resources\\n\\nDelete the resources created by this tutorial using the following SQL code:\\n\\n```sql title=\\"psql postgres://qpp3pzqb:LN6jnHfhRJTBD6ia@localhost:5432/_3ba201e349a11daf -c\\"\\nDELETE FROM repository_image WHERE private_repository_id = (SELECT id FROM repository WHERE repository_name = \'quickstart-repository\');\\nDELETE FROM ecs_simplified WHERE app_name = \'quickstart\';\\n```"},{"id":"/fargate","metadata":{"permalink":"/blog/fargate","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/tutorials/sql.mdx","source":"@site/blog/tutorials/sql.mdx","title":"Deploy containerized app to ECS Fargate","description":"In this tutorial, we will run SQL queries on an IaSQL database to deploy a Node.js HTTP server within a docker container on your AWS account using Fargate ECS, IAM, ECR, and ELB. The container image will be built locally, hosted within a private repository in ECR, and deployed to ECS using Fargate.","date":"2022-12-14T00:00:00.000Z","formattedDate":"December 14, 2022","tags":[{"label":"tutorial","permalink":"/blog/tags/tutorial"}],"readingTime":4.04,"hasTruncateMarker":true,"authors":[],"frontMatter":{"sidebar_position":1,"date":"2022-12-14T00:00:00.000Z","title":"Deploy containerized app to ECS Fargate","slug":"/fargate","tags":["tutorial"]},"prevItem":{"title":"Deploy containerized app to Fargate (Prisma)","permalink":"/blog/prisma"},"nextItem":{"title":"UPDATE iasql SET source = \'open\';","permalink":"/blog/os-iasql"}},"content":"In this tutorial, we will run SQL queries on an IaSQL [database](/docs/database) to deploy a Node.js HTTP server within a docker container on your AWS account using Fargate ECS, IAM, ECR, and ELB. The container image will be built locally, hosted within a private repository in ECR, and deployed to ECS using Fargate.\\n\x3c!--truncate--\x3e\\nIaSQL is an [open-source](https://github.com/alantech/iasql) software tool that creates a two-way connection between an unmodified PostgreSQL database and an AWS account so you can manage your infrastructure from a database.\\n\\n## Start managing an AWS account with a PostgreSQL IaSQL db\\n\\nFirst, make sure you have an [IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html) in AWS or create one with **Programmatic access** through the [console/UI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) or [CLI](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_cliwpsapi). Ensure that the IAM role has sufficient permissions to deploy and manage all your infrastructure resources.\\n\\nThere are two parts to each [access key](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys), which you\u2019ll see in the IAM console/CLI after you create it, an id and a secret. Input these in the connect account modal:\\n\\nimport useBaseUrl from \'@docusaurus/useBaseUrl\';\\nimport ThemedImage from \'@theme/ThemedImage\';\\n\\n<ThemedImage\\n  alt=\\"Connect\\"\\n  style={{width: \'440\'}}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/connect.png\'),\\n    dark: useBaseUrl(\'/screenshots/connect_dark.png\'),\\n  }}\\n/>\\n\\nIf you use the [AWS CLI](https://docs.aws.amazon.com/cli/), you can look at the [credentials configured locally](https://docs.aws.amazon.com/sdkref/latest/guide/file-location.html). In macOS and Linux this is as simple as:\\n\\n```bash\\n$ cat ~/.aws/credentials\\n[default]\\naws_access_key_id = <YOUR_ACCESS_KEY_ID>\\naws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>\\n```\\nYou will be able to see your PostgreSQL connection information when you press Connect.\\n\\n<ThemedImage\\n   alt=\\"Credentials\\"\\n   style={{width: \'440\', border: \'2px solid rgba(52, 52, 52, 0.1)\' }}\\n   sources={{\\n   light: useBaseUrl(\'/screenshots/credentials.png\'),\\n   dark: useBaseUrl(\'/screenshots/credentials_dark.png\'),\\n   }}\\n/>\\n\\n\x3c!-- TODO after release [connect](/docs/postgres) --\x3e\\nIf you want to connect to the PostgreSQL database outside of the IaSQL dashboard SQL editor, make sure to copy the PostgreSQL connection string as you will not see it again.\\n\\n## Add the necessary cloud services to the PostgreSQL database\\n\\nUse the `iasql_install` SQL function to install [modules](/docs/module) into the PostgreSQL database.\\n\\n```sql\\nSELECT * from iasql_install(\\n   \'aws_ecs_simplified\'\\n);\\n```\\n\\nIf the function call is successful, it will return a virtual table with a record for each new table in your database under `created_table_name` and the number of existing resources or records imported from the account under `record_count`.\\n\\n```sql\\n       module_name        |      created_table_name       | record_count\\n--------------------------+-------------------------------+--------------\\n aws_cloudwatch           | log_group                     |            0\\n aws_iam                  | iam_role                      |            0\\n aws_ecr                  | public_repository             |            0\\n aws_ecr                  | repository                    |            1\\n aws_ecr                  | repository_policy             |            0\\n aws_security_group       | security_group                |            2\\n aws_security_group       | security_group_rule           |            0\\n aws_vpc                  | vpc                           |            1\\n aws_vpc                  | subnet                        |            3\\n aws_elb                  | load_balancer                 |            0\\n aws_elb                  | target_group                  |            0\\n aws_elb                  | listener                      |            0\\n aws_elb                  | load_balancer_security_groups |            0\\n aws_ecs_fargate          | cluster                       |            0\\n aws_ecs_fargate          | service                       |            0\\n aws_ecs_fargate          | task_definition               |            0\\n aws_ecs_fargate          | container_definition          |            0\\n aws_ecs_fargate          | service_security_groups       |            0\\n aws_ecs_simplified       | ecs_simplified                |            0\\n(17 rows)\\n```\\n\\n## Provision cloud resources in your AWS account\\n\\nInsert a row into the `ecs_simplified` table within an IaSQL [`transaction`](/docs/transaction) the changes described in the PostgreSQL db to your cloud account which will take a few minutes waiting for AWS\\n\\n\\n```sql\\nSELECT * from iasql_begin();\\n\\nINSERT INTO ecs_simplified (app_name, app_port, public_ip, image_tag)\\nVALUES (\'quickstart\', 8088, true, \'latest\');\\n\\nSELECT * from iasql_commit();\\n```\\n\\nIf the function call is successful, it will return a virtual table with a record for each cloud resource that has been created, deleted, or updated.\\nLogin, build and push your code to the container registry\\n\\nPreviously, you needed to manually build and push your image to the ECR. But recently we\'ve added the high-level `ecr_build` SQL function which does all those steps automatically. It will do the following:\\n- Pull the code from your Github repository\\n- Build the Docker image in the directory you\'ve specified\\n- Push the image to the ECR repository you\'ve provided\\n\\nAll of these steps will be done in a CodeBuild project in your AWS account. To use the `ecr_build` function, you can run:\\n\\n```sql\\nSELECT ecr_build(\\n   \'https://github.com/alantech/iasql/\', -- replace with your own Github repo if you want to use your own codebase\\n   (SELECT id\\n    FROM repository\\n    WHERE repository_name = \'quickstart-repository\')::varchar(255), -- replace quickstart if you\'ve changed the project name\\n   \'./examples/ecs-fargate/prisma/app\', -- the sub directory in the Github repo that the image should be built in\\n   \'main\', -- the Github repo branch name\\n   \'\' -- replace your github personal access token here if the repo is private\\n);\\n```\\n\\nAfter running the above SQL command to completion, you can check the running app using the load balancer DNS name. To grab the name, run:\\n```bash\\nQUICKSTART_LB_DNS=$(psql -At \'postgres://d0va6ywg:nfdDh#EP4CyzveFr@localhost:5432/_4b2bb09a59a411e4\' -c \\"\\nSELECT dns_name\\nFROM load_balancer\\nWHERE load_balancer_name = \'<project-name>-load-balancer\';\\")\\n```\\nAnd then connect to your service!\\n\\n```\\ncurl ${QUICKSTART_LB_DNS}:8088/health\\n```\\n\\n\\n## Delete Managed Cloud Resources\\n\\nDelete the resources created by this tutorial using the following SQL code:\\n\\n```sql title=\\"psql postgres://qpp3pzqb:LN6jnHfhRJTBD6ia@localhost:5432/_3ba201e349a11daf -c\\"\\nDELETE FROM repository_image WHERE private_repository_id = (SELECT id FROM repository WHERE repository_name = \'quickstart-repository\');\\nDELETE FROM ecs_simplified WHERE app_name = \'quickstart\';\\n```"},{"id":"os-iasql","metadata":{"permalink":"/blog/os-iasql","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/updates/open-source.mdx","source":"@site/blog/updates/open-source.mdx","title":"UPDATE iasql SET source = \'open\';","description":"We are excited to announce that IaSQL is now open source! The main repository is https://github.com/iasql/iasql. As perfectionists, we feel like IaSQL will never be truly ready. However, we believe IaSQL is at the point where it can start to be useful for developers managing infrastructure in the cloud. IaSQL is a SaaS that lets you model your infrastructure as data by maintaining a 2-way connection between your AWS account and a Postgres SQL database to represent the definitive state (and status) of your cloud which cannot be achieved with a static infrastructure declaration. This means that when you connect an AWS account to an IaSQL instance it automatically backfills the database with your existing cloud resources. No need to painstakingly redefine or reconcile your existing infrastructure and IaSQL\'s module system means you can specify which parts of your cloud infrastructure you wish to control.","date":"2022-04-14T00:00:00.000Z","formattedDate":"April 14, 2022","tags":[{"label":"updates","permalink":"/blog/tags/updates"}],"readingTime":1.88,"hasTruncateMarker":true,"authors":[{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"},{"name":"David Ellis","imageURL":"https://github.com/dfellis.png","key":"dfellis"},{"name":"Alejandro Guillen","imageURL":"https://github.com/aguillenv.png","key":"aguillenv"}],"frontMatter":{"slug":"os-iasql","title":"UPDATE iasql SET source = \'open\';","date":"2022-04-14T00:00:00.000Z","tags":["updates"],"authors":["depombo","dfellis","aguillenv"]},"prevItem":{"title":"Deploy containerized app to ECS Fargate","permalink":"/blog/fargate"},"nextItem":{"title":"Introduction to Infrastructure as SQL","permalink":"/blog/intro-iasql"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\nimport ThemedImage from \'@theme/ThemedImage\';\\n\\nWe are excited to announce that [IaSQL](https://iasql.com) is now open source! The main repository is https://github.com/iasql/iasql. As perfectionists, we feel like IaSQL will never be truly ready. However, we believe IaSQL is at the point where it can start to be useful for developers managing infrastructure in the cloud. IaSQL is a SaaS that lets you model your infrastructure as data by maintaining a 2-way connection between your AWS account and a Postgres SQL database to represent the definitive state (and status) of your cloud which cannot be achieved with a static infrastructure declaration. This means that when you connect an AWS account to an IaSQL instance it automatically backfills the database with your existing cloud resources. No need to painstakingly redefine or reconcile your existing infrastructure and IaSQL\'s [module system](https://docs.iasql.com/module/) means you can specify which parts of your cloud infrastructure you wish to control.\\n\\n\x3c!--truncate--\x3e\\n\\n<ThemedImage\\n  alt=\\"Connector\\"\\n  style={{height: \'25rem\'}}\\n  sources={{\\n    light: useBaseUrl(\'/img/iasql-connector.gif\'),\\n    dark: useBaseUrl(\'/img/iasql-connector_dark.gif\'),\\n  }}\\n/>\\n\\nIaSQL also makes it possible to express infrastructure changes as code that can be version controlled. This can be done with any [migration system](https://en.wikipedia.org/wiki/Schema_migration) for schema and data changes, or in a script using [idempotent SQL inserts](https://www.prisma.io/dataguide/postgresql/inserting-and-modifying-data/insert-on-conflict) more akin to [IaC](https://en.wikipedia.org/wiki/Infrastructure_as_code).\\n\\nPostgreSQL IaSQL databases can be provisioned and configured locally or via our hosted offering. The dashboard calls the [IaSQL container](https://github.com/iasql/iasql) which is a Node.js server written in Typescript that provisions unmodified PostgreSQL instances loaded with tables representing AWS services controlled via the [AWS SDK](https://www.npmjs.com/package/aws-sdk). AWS is our main focus at the moment, but we plan to support GCP, Azure and other cloud providers soon. This is an [updated list](https://github.com/iasql/iasql#cloud-providers) of the AWS services that we currently support. Let us know if you need a specific AWS service and we should be able prioritize it!\\n\\n<ThemedImage\\n  alt=\\"Dashboard\\"\\n  style={{border: \'1px solid rgba(52, 52, 52, 0.1)\' }}\\n  sources={{\\n    light: useBaseUrl(\'/screenshots/dashboard.png\'),\\n    dark: useBaseUrl(\'/screenshots/dashboard_dark.png\'),\\n  }}\\n/>\\n\\n\\nWe want to make it easier to write IaSQL modules, reduce waiting times when provisioning infrastructure, add more functionality to the existing AWS services, and so on. The list of things we want to build into IaSQL is long, but we want to do it in the open with your feedback and help. Drop us a line on [Discord](https://discord.iasql.com)!"},{"id":"intro-iasql","metadata":{"permalink":"/blog/intro-iasql","editUrl":"https://github.com/alantech/iasql/tree/main/site/blog/updates/intro.mdx","source":"@site/blog/updates/intro.mdx","title":"Introduction to Infrastructure as SQL","description":"Since this post was originally written the schema of IaSQL has changed and the queries below no longer work. Please refer to the documentation for how to use IaSQL. The IaSQL modules include example queries that are tested and kept up-to-date, including the awsec2 module that this post covers.","date":"2021-09-14T00:00:00.000Z","formattedDate":"September 14, 2021","tags":[{"label":"updates","permalink":"/blog/tags/updates"}],"readingTime":3.855,"hasTruncateMarker":true,"authors":[{"name":"L. Fernando De Pombo","imageURL":"https://github.com/depombo.png","key":"depombo"},{"name":"David Ellis","imageURL":"https://github.com/dfellis.png","key":"dfellis"},{"name":"Alejandro Guillen","imageURL":"https://github.com/aguillenv.png","key":"aguillenv"}],"frontMatter":{"slug":"intro-iasql","title":"Introduction to Infrastructure as SQL","date":"2021-09-14T00:00:00.000Z","tags":["updates"],"authors":["depombo","dfellis","aguillenv"]},"prevItem":{"title":"UPDATE iasql SET source = \'open\';","permalink":"/blog/os-iasql"}},"content":":::caution\\nSince this post was originally written the schema of IaSQL has changed and the queries below no longer work. Please refer to the [documentation](/docs) for how to use IaSQL. The IaSQL modules include example queries that are tested and kept up-to-date, including the [`aws_ec2` module](/docs/modules/aws/aws_ec2/) that this post covers.\\n:::\\n\\nWhat software you have deployed on what services and the interactions between them and the outside world is not a program, it is information about your infrastructure. Changing your infrastructure\xa0*is*\xa0a set of operations to perform, a program. A SQL database is a set of information and SQL queries read or change that data.\\n\\n\x3c!--truncate--\x3e\\n\\n**Infrastructure State is Data, Infrastructure Change is Code. It\'s as simple as that.**\\n\\nAnd manipulating your infrastructure in this way is natural.\\n\\n```sql\\nINSERT INTO aws_ec2 (ami_id, ec2_instance_type_id)\\nSELECT ami.id, ait.id\\nFROM ec2_instance_type as ait, (\\n    SELECT id\\n    FROM   amis\\n    WHERE  image_name LIKE \'amzn-ami-hvm-%\'ORDER BY creation_date DESC\\n    LIMIT 1\\n) as ami\\nWHERE  ait.instance_name = \'t2.micro\';\\n```\\n\\n## **Relations and Types Matter for Infrastructure**\\n\\nInfrastructure as Code solutions do not have a good way of encoding dependencies across infrastructure pieces in a micro services architecture which makes it really hard to make and revert changes to infrastructure.\\n\\nRepresenting your infrastructure as SQL resolves the primary issue of YAML-based infrastructure tools by making the relations between pieces of your infrastructure first-class citizens, and enforcing type safety on the data and changes to it.\\n\\nYou can\'t set the EC2 instance type as\xa0`t2.mucro`\xa0and have your deploy system try and fail to create such an instance. The\xa0`insert`\xa0statement will fail and tell you zero rows were inserted and you can quickly see why.\\n\\nSimilarly, if you have a record in the\xa0`security_group`\xa0table, you can\'t delete it if there are any references to it in the\xa0`ec2_security_groups`\xa0join table. The relational structure of IaSQL prevents you from putting your infrastructure into an invalid state.\\n\\n## **New Powers: Explore, Query, and Automate Your Infrastructure**\\n\\nBecause your infrastructure is presented as a SQL database, you can connect to it with a SQL client of your choice and explore what you have and what the possibilities are.\\n\\n```sql\\nSHOW tables;\\n```\\n\\nYou can query for unusual usage patterns.\\n\\n```sql\\nSELECT aws_ec2.*\\nFROM aws_ec2\\nINNER JOIN ec2_instance_type AS ait ON ait.id = aws_ec2.ec2_instance_type_id\\nWHERE ait.vcpus > 8\\nORDER BY ait.vcpus DESC\\n```\\n\\nAnd since it is a database, you can create your own tables with their own meaning and associate them with your infrastructure.\\n\\n```sql\\nSELECT aws_ec2.*\\nFROM aws_ec2\\nINNER JOIN company_team_ec2s AS cte ON cte.aws_ec2_id = aws_ec2.id\\nINNER JOIN company_teams AS ct ON ct.id = cte.company_team_id\\nWHERE ct.name = \'Data Engineering\'\\n```\\n\\nFinally, your applications can know much more about what infrastructure they need than any auto-scaler solution out there. If you had a very infrequent but CPU/GPU-intensive job you need to handle at an unknown interval, you could give your application access to your IaSQL database and let it temporarily create and then destroy those resources.\\n\\n```jsx\\nconst ec2_instance_id = await iasql(`\\n  INSERT INTO aws_ec2 (ami_id, ec2_instance_type_id)\\n  SELECT ami.id, ait.id\\n  FROM ec2_instance_type as ait, (\\n      SELECT id\\n      FROM amis\\n      WHERE image_name = \'application-job-runner\'\\n  ) as ami\\n  WHERE ait.instance_name = \'g3.4xlarge\'\\n  RETURNING id;\\n`);\\nawait iasql(`\\n  INSERT INTO ec2_security_groups (ec2_id, security_group_id)\\n  SELECT ${ec2_instance_id}, sg.id\\n  FROM security_groups AS sg\\n  WHERE sg.name = \'application-job-group\';\\n`);\\n// Only large-enough job runners will take it based on job metadata\\nconst result = await job.run(myMassiveJob); \\nawait iasql(`\\n  DELETE FROM aws_ec2\\n  WHERE id = ${ec2_instance_id};\\n`);\\n```\\n\\n## **You Don\'t Need to Learn a New API (Probably)**\\n\\nNearly all cloud backend systems depend on a database, and most likely a SQL database, so you do not need to learn a new language to manipulate the infrastructure in this way.\\n\\nAnd likely you\'re using a\xa0[migration system](https://en.wikipedia.org/wiki/Schema_migration)\xa0in your backend to review changes to your database, which you can continue to use here, making it code to be reviewed, just like Infrastructure-as-Code.\\n\\n## **You Can Test, Too**\\n\\nSince the safety guarantees are provided by the types and relations between tables, you can simply copy your production infrastructure database into a local database and run your changes/migration against that and verify it works before you run it on your actual Infrastructure-as-SQL database.\\n\\n## **Recover With Ease**\\n\\nIt\'s 3AM and your service has gone down. You reverted the most recent IaSQL migration, but that didn\'t resolve the issue, and you aren\'t sure which change across which service today caused the outage. So, you simply replace the state of the IaSQL database with a snapshot from yesterday to bring everything back online to a known-good-state, and then take your time after you\'re well-rested to figure out what actually went wrong."}]}')}}]);